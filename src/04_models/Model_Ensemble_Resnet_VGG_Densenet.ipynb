{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afe4ea9d-d029-464f-8f99-04ab7e31da81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "device = 'cpu' \n",
    "if torch.cuda.is_available(): \n",
    " device='cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    " device = 'mps' \n",
    "\n",
    "print(\"device = \", device)\n",
    "DEVICE = torch.device(device)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print('CUDA is available. Working on GPU')\n",
    "#     DEVICE = torch.device('cuda')\n",
    "# else:\n",
    "#     print('CUDA is not available. Working on CPU')\n",
    "#     DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1345870f-fd0a-46d6-bfcf-18c7023cdcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, model_name, num_epochs, train_dataloader, val_dataloader):\n",
    "\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.33)\n",
    "\n",
    "    train_loss_array = []\n",
    "    train_acc_array = []\n",
    "    val_loss_array = []\n",
    "    val_acc_array = []\n",
    "    lowest_val_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "        print('Epoch: {} | Learning rate: {}'.format(epoch + 1, scheduler.get_lr()))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "\n",
    "            epoch_loss = 0\n",
    "            epoch_correct_items = 0\n",
    "            epoch_items = 0\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                with torch.enable_grad():\n",
    "                    for samples, targets in train_dataloader:\n",
    "                        samples = samples.to(DEVICE)\n",
    "                        targets = targets.to(DEVICE)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(samples)\n",
    "                        loss = loss_function(outputs, targets)\n",
    "                        preds = outputs.argmax(dim=1)\n",
    "                        correct_items = (preds == targets).float().sum()\n",
    "                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        epoch_loss += loss.item()\n",
    "                        epoch_correct_items += correct_items.item()\n",
    "                        epoch_items += len(targets)\n",
    "\n",
    "                train_loss_array.append(epoch_loss / epoch_items)\n",
    "                train_acc_array.append(epoch_correct_items / epoch_items)\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "            elif phase == 'val':\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for samples, targets in val_dataloader:\n",
    "                        samples = samples.to(DEVICE)\n",
    "                        targets = targets.to(DEVICE)\n",
    "\n",
    "                        outputs = model(samples)\n",
    "                        loss = loss_function(outputs, targets)\n",
    "                        preds = outputs.argmax(dim=1)\n",
    "                        correct_items = (preds == targets).float().sum()\n",
    "\n",
    "                        epoch_loss += loss.item()\n",
    "                        epoch_correct_items += correct_items.item()\n",
    "                        epoch_items += len(targets)\n",
    "\n",
    "                val_loss_array.append(epoch_loss / epoch_items)\n",
    "                val_acc_array.append(epoch_correct_items / epoch_items)\n",
    "\n",
    "                if epoch_loss / epoch_items < lowest_val_loss:\n",
    "                    lowest_val_loss = epoch_loss / epoch_items\n",
    "                    torch.save(model.state_dict(), '{}_weights.pth'.format(model_name))\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    print(\"\\t| New lowest val loss for {}: {}\".format(model_name, lowest_val_loss))\n",
    "\n",
    "    return best_model, train_loss_array, train_acc_array, val_loss_array, val_acc_array\n",
    "    \n",
    "    \n",
    "def visualize_training_results(train_loss_array,\n",
    "                               val_loss_array,\n",
    "                               train_acc_array,\n",
    "                               val_acc_array,\n",
    "                               num_epochs,\n",
    "                               model_name,\n",
    "                               batch_size):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14,4))\n",
    "    fig.suptitle(\"{} training | Batch size: {}\".format(model_name, batch_size), fontsize = 16)\n",
    "    axs[0].plot(list(range(1, num_epochs+1)), train_loss_array, label=\"train_loss\")\n",
    "    axs[0].plot(list(range(1, num_epochs+1)), val_loss_array, label=\"val_loss\")\n",
    "    axs[0].legend(loc='best')\n",
    "    axs[0].set(xlabel='epochs', ylabel='loss')\n",
    "    axs[1].plot(list(range(1, num_epochs+1)), train_acc_array, label=\"train_acc\")\n",
    "    axs[1].plot(list(range(1, num_epochs+1)), val_acc_array, label=\"val_acc\")\n",
    "    axs[1].legend(loc='best')\n",
    "    axs[1].set(xlabel='epochs', ylabel='accuracy')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8822227e-97d9-423e-8b2e-008954fd8c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12e3f390-4b59-4e1b-b9d7-bda70a8afb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2,p=0.5),\n",
    "    transforms.RandomAutocontrast(p=0.5),\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    transforms.Resize(size=(224, 224)),    \n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    transforms.Resize(size=(224, 224)),    \n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68441269-ea86-42b6-acfe-cec71459eed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set records :  64454\n",
      "Validation set records :  22107\n",
      "Test set records :  19299\n"
     ]
    }
   ],
   "source": [
    "train_all_dataset = dset.ImageFolder(os.path.join(ataroot,'train', 'img_mask_ov_crop_class'), transform=train_transform)\n",
    "valid_all_dataset = dset.ImageFolder(os.path.join( ataroot, 'hold' ,'img_mask_ov_crop_class'), transform=valid_transform)\n",
    "test_all_dataset = dset.ImageFolder(os.path.join( ataroot, 'test' , 'img_mask_ov_crop_class'), transform=test_transform)\n",
    "print(\"Train set records : \", len(train_all_dataset))\n",
    "print(\"Validation set records : \", len(valid_all_dataset))\n",
    "print(\"Test set records : \", len(test_all_dataset))\n",
    "\n",
    "# trainset_all = torch.utils.data.Subset(train_all_dataset, list(range(len(train_all_dataset))))\n",
    "# testset_all = torch.utils.data.Subset(test_all_dataset, list(range(len(test_all_dataset))))\n",
    "# validset_all = torch.utils.data.Subset(valid_all_dataset, list(range(len(valid_all_dataset))))\n",
    "\n",
    "#sampler = torch.utils.data.sampler.WeightedRandomSampler(element_weights, num_epoch_elements, replacement=False)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_all = DataLoader(train_all_dataset, batch_size=batch_size, num_workers=2)\n",
    "valid_loader_all = DataLoader(valid_all_dataset, batch_size=batch_size, num_workers=2)\n",
    "test_loader_all = DataLoader(test_all_dataset, batch_size=batch_size, num_workers= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69499c0d-8cd1-4b86-9b18-c4f19ae0b3fe",
   "metadata": {},
   "source": [
    "### Densenet 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381043c1-d0ce-4d68-84fb-3b875ad26a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_densenet161 = models.densenet161(pretrained=True)\n",
    "for param in model_densenet161.parameters():\n",
    "    param.requires_grad = False\n",
    "model_densenet161.classifier = torch.nn.Linear(model_densenet161.classifier.in_features, out_features=200)\n",
    "model_densenet161 = model_densenet161.to(DEVICE)\n",
    "\n",
    "densenet161_training_results = training(model=model_densenet161,\n",
    "                                        model_name='DenseNet161',\n",
    "                                        num_epochs=num_epochs,\n",
    "                                        train_dataloader=train_dataloader,\n",
    "                                        val_dataloader=val_dataloader)\n",
    "\n",
    "model_densenet161, train_loss_array, train_acc_array, val_loss_array, val_acc_array = densenet161_training_results\n",
    "\n",
    "min_loss = min(val_loss_array)\n",
    "min_loss_epoch = val_loss_array.index(min_loss)\n",
    "min_loss_accuracy = val_acc_array[min_loss_epoch]\n",
    "\n",
    "visualize_training_results(train_loss_array,\n",
    "                           val_loss_array,\n",
    "                           train_acc_array,\n",
    "                           val_acc_array,\n",
    "                           num_epochs,\n",
    "                           model_name=\"DenseNet161\",\n",
    "                           batch_size=64)\n",
    "print(\"\\nTraining results:\")\n",
    "print(\"\\tMin val loss {:.4f} was achieved during epoch #{}\".format(min_loss, min_loss_epoch + 1))\n",
    "print(\"\\tVal accuracy during min val loss is {:.4f}\".format(min_loss_accuracy))a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4e7d3-58cd-45d5-b627-f2c6a29df326",
   "metadata": {},
   "source": [
    "### Resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112de162-e249-4008-a101-230665d0607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_resnet152 = models.resnet152(pretrained=True)\n",
    "for param in model_resnet152.parameters():\n",
    "    param.requires_grad = False\n",
    "model_resnet152.fc = torch.nn.Linear(model_resnet152.fc.in_features, 200)\n",
    "model_resnet152 = model_resnet152.to(DEVICE)\n",
    "\n",
    "resnet152_training_results = training(model=model_resnet152,\n",
    "                                      model_name='ResNet152',\n",
    "                                      num_epochs=num_epochs,\n",
    "                                      train_dataloader=train_dataloader,\n",
    "                                      val_dataloader=val_dataloader)\n",
    "\n",
    "model_resnet152, train_loss_array, train_acc_array, val_loss_array, val_acc_array = resnet152_training_results\n",
    "\n",
    "min_loss = min(val_loss_array)\n",
    "min_loss_epoch = val_loss_array.index(min_loss)\n",
    "min_loss_accuracy = val_acc_array[min_loss_epoch]\n",
    "\n",
    "visualize_training_results(train_loss_array,\n",
    "                           val_loss_array,\n",
    "                           train_acc_array,\n",
    "                           val_acc_array,\n",
    "                           num_epochs,\n",
    "                           model_name=\"ResNet152\",\n",
    "                           batch_size=64)\n",
    "print(\"\\nTraining results:\")\n",
    "print(\"\\tMin val loss {:.4f} was achieved during epoch #{}\".format(min_loss, min_loss_epoch + 1))\n",
    "print(\"\\tVal accuracy during min val loss is {:.4f}\".format(min_loss_accuracy))\n",
    "view rawEnsemble_ImageClassification_TinyImageNet.py hosted with ❤ by GitHuba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff283e-eb25-4e0b-ba49-a8565da12ba5",
   "metadata": {},
   "source": [
    "### VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2e5ff4f-75a2-4396-8c83-eee92f3a5ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaminigotimukul/anaconda3/envs/alivio-tune/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/yaminigotimukul/anaconda3/envs/alivio-tune/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to /Users/yaminigotimukul/.cache/torch/hub/checkpoints/vgg19_bn-c79401a0.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 548M/548M [00:08<00:00, 64.6MB/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m model_vgg19_bn\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39mmodel_vgg19_bn\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m.\u001b[39min_features, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m      5\u001b[0m model_vgg19_bn \u001b[38;5;241m=\u001b[39m model_vgg19_bn\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      7\u001b[0m vgg19_bn_training_results \u001b[38;5;241m=\u001b[39m training(model\u001b[38;5;241m=\u001b[39mmodel_vgg19_bn,\n\u001b[1;32m      8\u001b[0m                                      model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVGG19_bn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m                                      num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[0;32m---> 10\u001b[0m                                      train_dataloader\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataloader\u001b[49m,\n\u001b[1;32m     11\u001b[0m                                      val_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader)\n\u001b[1;32m     13\u001b[0m model_vgg19_bn, train_loss_array, train_acc_array, val_loss_array, val_acc_array \u001b[38;5;241m=\u001b[39m vgg19_bn_training_results\n\u001b[1;32m     15\u001b[0m min_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(val_loss_array)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model_vgg19_bn = models.vgg19_bn(pretrained=True)\n",
    "for param in model_vgg19_bn.parameters():\n",
    "    param.requires_grad = False\n",
    "model_vgg19_bn.classifier[6] = torch.nn.Linear(in_features=model_vgg19_bn.classifier[6].in_features, out_features=200)\n",
    "model_vgg19_bn = model_vgg19_bn.to(DEVICE)\n",
    "\n",
    "vgg19_bn_training_results = training(model=model_vgg19_bn,\n",
    "                                     model_name='VGG19_bn',\n",
    "                                     num_epochs=num_epochs,\n",
    "                                     train_dataloader=train_dataloader,\n",
    "                                     val_dataloader=val_dataloader)\n",
    "\n",
    "model_vgg19_bn, train_loss_array, train_acc_array, val_loss_array, val_acc_array = vgg19_bn_training_results\n",
    "\n",
    "min_loss = min(val_loss_array)\n",
    "min_loss_epoch = val_loss_array.index(min_loss)\n",
    "min_loss_accuracy = val_acc_array[min_loss_epoch]\n",
    "\n",
    "visualize_training_results(train_loss_array,\n",
    "                           val_loss_array,\n",
    "                           train_acc_array,\n",
    "                           val_acc_array,\n",
    "                           num_epochs,\n",
    "                           model_name=\"VGG19_bn\",\n",
    "                           batch_size=64)\n",
    "print(\"\\nTraining results:\")\n",
    "print(\"\\tMin val loss {:.4f} was achieved during epoch #{}\".format(min_loss, min_loss_epoch + 1))\n",
    "print(\"\\tVal accuracy during min val loss is {:.4f}\".format(min_loss_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b78d6-69df-41be-ae21-dd37026ea047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):   \n",
    "    def __init__(self, modelA, modelB, modelC):\n",
    "        super().__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.modelC = modelC\n",
    "        self.classifier = nn.Linear(200 * 3, 200)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.modelA(x)\n",
    "        x2 = self.modelB(x)\n",
    "        x3 = self.modelC(x)\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        out = self.classifier(x)\n",
    "        return out\n",
    "    \n",
    "ensemble_model = EnsembleModel(model_densenet161, model_resnet152, model_vgg19_bn)\n",
    "\n",
    "for param in ensemble_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in ensemble_model.classifier.parameters():\n",
    "    param.requires_grad = True    \n",
    "\n",
    "ensemble_model = ensemble_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb997e-f5cc-4bbd-912a-58249084e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ensemble_training_results = training(model=ensemble_model,\n",
    "                                     model_name='Ensemble',\n",
    "                                     num_epochs=20,\n",
    "                                     train_dataloader=train_dataloader,\n",
    "                                     val_dataloader=val_dataloader)\n",
    "\n",
    "ensemble_model, train_loss_array, train_acc_array, val_loss_array, val_acc_array = ensemble_training_results\n",
    "\n",
    "min_loss = min(val_loss_array)\n",
    "min_loss_iteration = val_loss_array.index(min_loss)\n",
    "min_loss_accuracy = val_acc_array[min_loss_iteration]\n",
    "\n",
    "visualize_training_results(train_loss_array,\n",
    "                           val_loss_array,\n",
    "                           train_acc_array,\n",
    "                           val_acc_array,\n",
    "                           num_epochs=20,\n",
    "                           model_name=\"Ensemble model\",\n",
    "                           batch_size=64)\n",
    "print(\"\\nTraining results:\")\n",
    "print(\"\\tMin val loss {:.4f} was achieved during iteration #{}\".format(min_loss, min_loss_iteration + 1))\n",
    "print(\"\\tVal accuracy during min val loss is {:.4f}\".format(min_loss_accuracy))b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454965d9-2a14-46e6-80e5-f29020917ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "all_preds = []\n",
    "all_files = []\n",
    "\n",
    "ensemble_model.eval()\n",
    "with torch.no_grad():\n",
    "    for samples, f_names in tqdm(test_dataloader):\n",
    "        samples = samples.to(DEVICE)\n",
    "        outputs = ensemble_model(samples)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_files.extend(f_names)\n",
    "\n",
    "all_filenames = [f_name[30:] for f_name in all_files]\n",
    "all_preds_decoded = encoder_labels.inverse_transform(all_preds)\n",
    "\n",
    "submission_ensemble_df = pd.DataFrame(list(zip(all_filenames, all_preds_decoded)), columns =['File', 'Prediction'])\n",
    "submission_ensemble_df.to_csv('test_predictions_ensemble.csv', header=False, index=False) \n",
    "submission_ensemble_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alivio-tune",
   "language": "python",
   "name": "alivio-tune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
