{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b263eaa2-8daf-439d-841a-eef9fd2d5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "from datetime import date\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from torch import tensor\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os import path, walk, makedirs\n",
    "from skimage.io import imread\n",
    "from shapely import wkt\n",
    "from shapely.geometry import mapping, MultiPolygon, Polygon\n",
    "from cv2 import fillPoly, imwrite\n",
    "from PIL import Image\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "import shapely.wkt\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torcheval.metrics.functional import multiclass_precision, multiclass_f1_score, multiclass_recall, multiclass_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c005a7b-910f-4000-af2e-dba3e09f0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44624c9b-a3eb-48a2-bdea-78b06a0c9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "INFERENCE_ROOT = os.path.join(ROOT_DIR, 'data','xview_building_damage','inference')\n",
    "UPLOADS_IMG = os.path.join(INFERENCE_ROOT, 'upload', 'img')\n",
    "UPLOADS_JSON = os.path.join(INFERENCE_ROOT, 'upload', 'json')\n",
    "POST_PROCESSED =os.path.join(INFERENCE_ROOT, 'postprocesssed')\n",
    "PRE_PROCESSED =os.path.join(INFERENCE_ROOT, 'preprocesssed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68eb6b26-4a81-490f-b5af-f0cada9f6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_imgs = (\n",
    "    [f\"{UPLOADS_IMG}/{f}\" for f in os.listdir(UPLOADS_IMG)]\n",
    ")\n",
    "uploaded_jsons = (\n",
    "    [f\"{UPLOADS_JSON}/{f}\" for f in os.listdir(UPLOADS_JSON)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8611bae-e329-463f-af36-ca3af18cdc2e",
   "metadata": {},
   "source": [
    "### Convert the Uploaded JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c136d9ae-c7a7-43c3-b480-53f9afde4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_json_data: list[dict] = []\n",
    "\n",
    "def read_and_store_label_json(label_json_path: str):\n",
    "    \"\"\"A thread-safe function that reads a json as a dictionary and writes to a global list\"\"\"\n",
    "    with open(label_json_path) as f:\n",
    "        label_json_data.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be8b2d0b-3c93-4b68-9cbd-b695154319eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_store_label_json(uploaded_jsons[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a3225eb-2ac7-42b0-ba6c-9ccb8ebe9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_json_series: pd.Series = pd.Series(label_json_data)\n",
    "label_df_original: pd.DataFrame = pd.json_normalize(label_json_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de497405-c9e5-4a36-ab09-0c1355b979b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lbl_df: pd.DataFrame = label_df_original.copy()\n",
    "CHALLENGE_TYPE: Literal[\"train\", \"test\", \"hold\"] = \"test\"\n",
    "\n",
    "def json_df_to_csv(label_df):\n",
    "    \n",
    "    label_df_lng_lat: pd.DataFrame = (\n",
    "        label_df.drop(columns=[\"features.xy\", \"features.lng_lat\"])\n",
    "        .join(label_df[\"features.lng_lat\"].explode())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    label_df_features: pd.DataFrame = (\n",
    "        label_df.drop(columns=[\"features.xy\", \"features.lng_lat\"])\n",
    "        .join(label_df[\"features.xy\"].explode())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    lng_lat_normalized: pd.DataFrame = pd.json_normalize(label_df_lng_lat[\"features.lng_lat\"]).rename(\n",
    "        columns={\n",
    "            \"wkt\": \"map_polygon\",\n",
    "            \"properties.feature_type\": \"map_feature_type\",\n",
    "            \"properties.subtype\": \"map_damage\",\n",
    "            \"properties.uid\": \"building_id\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    features_normalized: pd.DataFrame = pd.json_normalize(\n",
    "        label_df_features[\"features.xy\"]\n",
    "    ).rename(\n",
    "        columns={\n",
    "            \"wkt\": \"image_polygon\",\n",
    "            \"properties.feature_type\": \"image_feature_type\",\n",
    "            \"properties.subtype\": \"image_damage\",\n",
    "            \"properties.uid\": \"building_id\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    label_df_lng_lat_normalized = label_df_lng_lat.drop(columns=[\"features.lng_lat\"]).join(\n",
    "        lng_lat_normalized\n",
    "    )\n",
    "    \n",
    "    label_df_features_normalized = label_df_features.drop(columns=[\"features.xy\"]).join(\n",
    "        features_normalized\n",
    "    )\n",
    "    \n",
    "    label_df_final: pd.DataFrame = label_df_lng_lat_normalized.merge(\n",
    "        label_df_features_normalized[\n",
    "            [\n",
    "                \"metadata.id\",\n",
    "                \"image_polygon\",\n",
    "                \"image_feature_type\",\n",
    "                \"image_damage\",\n",
    "                \"building_id\",\n",
    "            ]\n",
    "        ],\n",
    "        \"left\",\n",
    "        [\"metadata.id\", \"building_id\"],\n",
    "    )\n",
    "    \n",
    "    label_df_final = (\n",
    "        label_df_final.rename(\n",
    "            columns={\n",
    "                c: c.replace(\"metadata.\", \"\")\n",
    "                for c in label_df_final.columns\n",
    "                if c.startswith(\"metadata.\")\n",
    "            }\n",
    "        )\n",
    "        .drop(\n",
    "            columns=[\n",
    "                \"map_feature_type\",\n",
    "                \"map_damage\",\n",
    "            ]\n",
    "        )\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"image_feature_type\": \"feature_type\",\n",
    "                \"image_damage\": \"damage\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    label_df_final[\"dataset\"] = CHALLENGE_TYPE\n",
    "    label_df_final[\"capture_date\"] = pd.to_datetime(label_df_final[\"capture_date\"])\n",
    "    \n",
    "    label_df_final[\"image_id\"] = label_df_final[\"img_name\"].dropna().apply(lambda cell: \"_\".join(cell.split(\"_\")[0:2]))\n",
    "    label_df_final[\"is_pre_image\"] = label_df_final[\"img_name\"].dropna().apply(lambda cell: \"_pre_disaster\" in cell)\n",
    "    label_df_final[\"is_post_image\"] = (\n",
    "        label_df_final[\"img_name\"].dropna().apply(lambda cell: \"_post_disaster\" in cell)\n",
    "    )\n",
    "    \n",
    "    label_df_final.to_parquet(f\"{CHALLENGE_TYPE}.parquet\")\n",
    "    \n",
    "    concat_list: list[pd.DataFrame] = [\n",
    "        pd.read_parquet(pq_file) for pq_file in os.listdir() if pq_file.endswith(\".parquet\")\n",
    "    ]\n",
    "    \n",
    "    df = pd.concat(concat_list).reset_index(drop=True)\n",
    "    df.to_parquet(os.path.join(POST_PROCESSED, 'inference_data.parquet'))\n",
    "    \n",
    "    df.to_csv(\n",
    "       os.path.join(POST_PROCESSED, 'inference_data.csv'), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bd256d4-9e61-4547-869e-60a5515cb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_to_csv(lbl_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f3e4b-51c7-4c03-8db9-6fd2d953c936",
   "metadata": {},
   "source": [
    "### Step 2 : Begin Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbcc570e-2f6d-4050-8409-cc66b599d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_class_numeric_labels(df_name):\n",
    "    # df_name['damage'].fillna('pre', inplace=True)\n",
    "    df_name['damage_class']=df_name['damage']\n",
    "    keys=list(df_name['damage_class'].value_counts().keys())\n",
    "    df_name['damage_class']=df_name['damage_class'].apply(keys.index)\n",
    "    df_name['damage_class'].value_counts()\n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ff4faa0-7b5a-4286-9f57-c8f62a8543ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    infer_csv = pd.read_csv(os.path.join(POST_PROCESSED,'inference_data.csv'))\n",
    "    data = infer_csv[infer_csv['image_polygon'].notna()]\n",
    "    df_disaster = data[data['damage'] != 'un-classified']\n",
    "    df_disaster['mask_file_names'] = df_disaster['img_name'].str.replace('.png', '_')+df_disaster['building_id']+'.png'\n",
    "    df_disaster_class_labels = get_df_with_class_numeric_labels(df_disaster)\n",
    "    return df_disaster_class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccf5c7fc-6c5b-4182-b744-fda36624259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_n/997tnw7n3tdg9z81cr8j96dc0000gn/T/ipykernel_34107/2942758920.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_disaster['mask_file_names'] = df_disaster['img_name'].str.replace('.png', '_')+df_disaster['building_id']+'.png'\n",
      "/var/folders/_n/997tnw7n3tdg9z81cr8j96dc0000gn/T/ipykernel_34107/1265940060.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_name['damage_class']=df_name['damage']\n",
      "/var/folders/_n/997tnw7n3tdg9z81cr8j96dc0000gn/T/ipykernel_34107/1265940060.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_name['damage_class']=df_name['damage_class'].apply(keys.index)\n"
     ]
    }
   ],
   "source": [
    "df=get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9316bfa4-a065-4a28-8b9b-c1d4e0f1ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygons_mask(polygons):\n",
    "    img_mask = np.zeros(im_size, np.uint8)\n",
    "    if not polygons:\n",
    "        return img_mask\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "    exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
    "    interiors = [int_coords(pi.coords) for poly in polygons\n",
    "                 for pi in poly.interiors]\n",
    "    cv2.fillPoly(img_mask, exteriors, 1)\n",
    "    cv2.fillPoly(img_mask, interiors, 0)\n",
    "    return img_mask\n",
    "\n",
    "def create_image_mask_overall(root_dir, im_size, meta_df):\n",
    "    input_dir =  os.path.join(root_dir, 'upload') \n",
    "    dest_dir = os.path.join(root_dir, str(date.today()))\n",
    "    img_input = os.path.join(input_dir, 'img')\n",
    "    \n",
    "    if os.path.exists(dest_dir):\n",
    "      print(\"Removing the dir with name: \", dest_dir) \n",
    "      os.system(\"rm -rf \"+dest_dir)\n",
    "        \n",
    "    print(\"creating empty dir with name \" , dest_dir)\n",
    "    os.makedirs(dest_dir)\n",
    "    \n",
    "    img_overlay = os.path.join(dest_dir, 'img_mask_overlay')\n",
    "    if os.path.exists(img_overlay):\n",
    "        print(\"Removing the dir with name: \", img_overlay) \n",
    "        os.system(\"rm -rf \"+img_overlay)\n",
    "    \n",
    "    print(\"creating empty dir with name \" , img_overlay)\n",
    "    os.makedirs(img_overlay)\n",
    "   \n",
    "   \n",
    "   #output_dir =os.path.join(root_dir, 'challenge', dataSplit, 'disaster','hurricanes-all', 'img_mask_overlay', hurricane_name )\n",
    "\n",
    "    df = meta_df[meta_df['is_post_image'] == True]\n",
    "    \n",
    "    print(\"Starting : Mask overlay\")\n",
    "    for idx, file_name in enumerate(df['mask_file_names']):\n",
    "       image = cv2.imread(os.path.join(img_input, df.iloc[idx]['img_name']))\n",
    "       mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "       _mask = polygons_mask([shapely.wkt.loads(df.iloc[idx]['image_polygon'])])\n",
    "       masked = cv2.bitwise_and(image, image, mask=_mask)\n",
    "       plt.imsave(os.path.join(img_overlay, file_name), masked)\n",
    "    print(\"Ending : Mask overlay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2115d5e6-58fa-494c-9b64-ddac0907883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds_tp(image_wkt):\n",
    "    bounds = wkt.loads(image_wkt).bounds\n",
    "    return (bounds[0], bounds[1], bounds[2], bounds[3]) ## \n",
    "\n",
    "def crop_save_masked_images(root_dir, meta_df, crop_output_dir_name = 'img_mask_overlay_crops'):\n",
    "    input_dir =  os.path.join(root_dir, str(date.today())) \n",
    "    img_crop_overlay = os.path.join(input_dir, crop_output_dir_name)\n",
    "    if os.path.exists(img_crop_overlay):\n",
    "        print(\"Removing the dir with name: \", img_crop_overlay) \n",
    "    os.system(\"rm -rf \"+img_crop_overlay)\n",
    "    \n",
    "    print(\"creating empty dir with name \" , img_crop_overlay)\n",
    "    os.makedirs(img_crop_overlay)\n",
    "    \n",
    "    print(\"Starting Cropping the images\")\n",
    "    for idx, file_name in enumerate(meta_df['mask_file_names']):\n",
    "        img = Image.open(os.path.join(input_dir,'img_mask_overlay', file_name))\n",
    "        minx, miny, maxx, maxy = get_bounds_tp(meta_df.iloc[idx]['image_polygon'])\n",
    "        cropped_img=img.crop((minx-5, miny-5, maxx+5, maxy+5))\n",
    "        cropped_img.save(os.path.join(img_crop_overlay, file_name))\n",
    "    print(\"Finished Cropping the images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "674b9458-6cfa-4ede-bebc-1fe4ddbfdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_masks_by_class(top_dir, meta_df, cls_path='img_mask_ov_crop_class'):\n",
    "    input_dir =  os.path.join(top_dir, str(date.today())) \n",
    "    disas_post_mask= os.path.join(input_dir, 'img_mask_overlay_crops') #source\n",
    "    print(\"Source root : \", disas_post_mask)\n",
    "    disas_class_path=os.path.join(input_dir, 'img_mask_ov_crop_class' )\n",
    "    if os.path.exists(disas_class_path):\n",
    "        print(\"Removing the dir with name: \", disas_class_path) \n",
    "    os.system(\"rm -rf \"+disas_class_path)\n",
    "    \n",
    "    print(\"creating empty dir with name \" , disas_class_path)\n",
    "    os.makedirs(disas_class_path)\n",
    "    \n",
    "    print(\"Destination root : \", disas_class_path)\n",
    "    \n",
    "    df = meta_df[meta_df['is_post_image'] == True]\n",
    "    \n",
    "    print(\"Started moving the mask files to class folder \")\n",
    "    for idx, file_name in enumerate(df['mask_file_names']):\n",
    "        source = os.path.join(disas_post_mask, df.iloc[idx]['mask_file_names'])\n",
    "        destination = os.path.join(disas_class_path, df.iloc[idx]['damage'])\n",
    "        if os.path.exists(destination):\n",
    "            pass\n",
    "        else:\n",
    "            print( \"Creating dir for \" , df.iloc[idx]['damage'])\n",
    "            os.makedirs(destination)\n",
    "        \n",
    "        if os.path.exists(source):\n",
    "            shutil.copy(source, destination)\n",
    "    print(\"Finshed moving the mask files to class folder \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1636bd1-9769-4c02-b7c8-47916bf1eed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the dir with name:  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02\n",
      "creating empty dir with name  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02\n",
      "creating empty dir with name  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_overlay\n",
      "Starting : Mask overlay\n",
      "Ending : Mask overlay\n"
     ]
    }
   ],
   "source": [
    "im_size =(1024, 1024)\n",
    "create_image_mask_overall(INFERENCE_ROOT, im_size, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e6c604e-1dd2-4ba7-8c23-34ea273ceffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the dir with name:  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_overlay_crops\n",
      "creating empty dir with name  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_overlay_crops\n",
      "Starting Cropping the images\n",
      "Finished Cropping the images\n"
     ]
    }
   ],
   "source": [
    "crop_save_masked_images(INFERENCE_ROOT, df, crop_output_dir_name = 'img_mask_overlay_crops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e54eb3b5-87f9-4cb8-9ed5-0d0de22089d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_overlay_crops\n",
      "creating empty dir with name  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_ov_crop_class\n",
      "Destination root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_ov_crop_class\n",
      "Started moving the mask files to class folder \n",
      "Creating dir for  no-damage\n",
      "Creating dir for  minor-damage\n",
      "Creating dir for  major-damage\n",
      "Creating dir for  destroyed\n",
      "Finshed moving the mask files to class folder \n"
     ]
    }
   ],
   "source": [
    "sort_masks_by_class(INFERENCE_ROOT , df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a821765b-c9f8-4608-a57f-e41411fee4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight_decay(batch, train_data_len, nepoches, lambda_norm):\n",
    "    return lambda_norm * math.sqrt((batch/(train_data_len * nepoches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef1286-4363-4372-b810-44d3eb3eeef7",
   "metadata": {},
   "source": [
    "### Test Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84a50805-577c-4e99-ada6-d790653d7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    transforms.Resize(size=(224, 224)),    \n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3f80886f-f34a-43e3-8f55-59bccbbde0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "inference_all_dataset = datasets.ImageFolder(os.path.join( INFERENCE_ROOT,'2024-04-02', 'img_mask_ov_crop_class'), transform=test_transform)\n",
    "inference_loader = DataLoader(inference_all_dataset, batch_size=batch_size, num_workers= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3db242d2-9f26-4764-acef-6e50ad81b540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_names  ['destroyed', 'major-damage', 'minor-damage', 'no-damage']\n",
      "class_dict  {'destroyed': 0, 'major-damage': 1, 'minor-damage': 2, 'no-damage': 3}\n"
     ]
    }
   ],
   "source": [
    "class_names = inference_all_dataset.classes\n",
    "print(\"class_names \",class_names)\n",
    "\n",
    "class_dict = inference_all_dataset.class_to_idx\n",
    "print(\"class_dict \",class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0058c-320f-4152-b97b-54456c6719a2",
   "metadata": {},
   "source": [
    "### Load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1efb13a3-9ec4-41ad-9d84-64079dee8491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== Model Layers ==========================\n",
      "Layer Name: class_token, Frozen: True\n",
      "\n",
      "Layer Name: conv_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: conv_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.pos_embedding, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.ln.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.ln.bias, Frozen: True\n",
      "\n",
      "Layer Name: heads.weight, Frozen: False\n",
      "\n",
      "Layer Name: heads.bias, Frozen: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/train_models\"\n",
    "vit_model=joblib.load(os.path.join(\"/Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/train_models\", \"vit2024-04-04-f155.pkl\"))\n",
    "print(\"=============================== Model Layers ==========================\")\n",
    "for layer_name, p in vit_model.named_parameters():\n",
    "    print('Layer Name: {}, Frozen: {}'.format(layer_name, not p.requires_grad))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9970f7c6-906f-4f45-8390-dcdf959da1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(labels):\n",
    "    class_counts = np.bincount(labels)\n",
    "    num_classes = len(class_counts)\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    class_weights = []\n",
    "    for count in class_counts:\n",
    "        weight = 1 / (count / total_samples)\n",
    "        class_weights.append(weight)\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a87a54b-4056-4cf8-b5b5-fb66ff699e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(preds_list, target_list, num_classes = 4 ):\n",
    "    pred_ts=tensor(preds_list)\n",
    "    target_ts = tensor(target_list)\n",
    "    \n",
    "    accuracy = multiclass_accuracy(pred_ts, target_ts, num_classes=4)\n",
    "    \n",
    "    f1_score = multiclass_f1_score(pred_ts, target_ts, num_classes=4, average=\"weighted\")\n",
    "    \n",
    "    precision = multiclass_precision(pred_ts, target_ts, num_classes=4, average=\"weighted\")\n",
    "    recall = multiclass_recall(pred_ts, target_ts, num_classes=4, average=\"weighted\")\n",
    "    \n",
    "    print(\"Accuracy :\", accuracy)\n",
    "    print(\"F1-score : \", f1_score)\n",
    "    print(\"Precision : \", precision)\n",
    "    print(\"Recall : \", recall)\n",
    "    return accuracy, f1_score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "95d63201-a072-41a9-ac2c-c26dc2b41760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(class_correct, class_total, class_names, accuracy ):\n",
    "    n_class = len(class_names)\n",
    "\n",
    "    class_accuracy = class_correct / class_total\n",
    "\n",
    "    print('Test Accuracy of Classes')\n",
    "    print()\n",
    "    \n",
    "    for c in range(n_class):\n",
    "        print('{}\\t: {}% \\t ({}/{})'.format(class_names[c],\n",
    "                                    int(class_accuracy[c] * 100), int(class_correct[c]), int(class_total[c])) )\n",
    "    \n",
    "    print()\n",
    "    print('Test Accuracy of Dataset: \\t {}% \\t ({}/{})'.format(int(accuracy),\n",
    "                                                               int(np.sum(class_correct)), int(np.sum(class_total)) ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fdd72248-1bfa-4568-bbbd-fda91e4a666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, test_loader, criterion, class_names ):\n",
    "    model.eval()\n",
    "    \n",
    "    preds_list = []\n",
    "    target_list = []\n",
    "    output_list = []\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    # test_focal_loss = 0.0\n",
    "    accuracy = 0\n",
    "    \n",
    "    # number of classes\n",
    "    n_class = len(class_names)\n",
    "    \n",
    "    class_correct = np.zeros(n_class)\n",
    "    class_total = np.zeros(n_class)\n",
    "    \n",
    "    # move model back to cpu\n",
    "    model = model.to('cpu')\n",
    "    \n",
    "    # test model\n",
    "    for images, targets in test_loader:\n",
    "    \n",
    "        # get outputs\n",
    "        outputs = model(images)\n",
    "    \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        # fl = get_focal_loss(loss)\n",
    "    \n",
    "        # track loss\n",
    "        test_loss += loss.item()\n",
    "        #test_focal_loss += fl.item()\n",
    "        \n",
    "        # get predictions from probabilities\n",
    "        preds = torch.argmax(F.softmax(outputs, dim=1), dim=1)\n",
    "    \n",
    "        target_list.extend(targets)\n",
    "        output_list.extend(torch.argmax(F.softmax(outputs, dim=1), dim=1 ))\n",
    "        preds_list.extend(preds)\n",
    "    \n",
    "        # get correct predictions\n",
    "        correct_preds = (preds == targets).type(torch.FloatTensor)\n",
    "    \n",
    "        # calculate and accumulate accuracy\n",
    "        accuracy += torch.mean(correct_preds).item() * 100\n",
    "    \n",
    "        # calculate test accuracy for each class\n",
    "        for c in range(n_class):\n",
    "    \n",
    "            targets = targets.to('cpu')\n",
    "    \n",
    "            class_total[c] += (targets == c).sum()\n",
    "            class_correct[c] += ((correct_preds) * (targets == c)).sum()\n",
    "    \n",
    "    # get average accuracy\n",
    "    accuracy = accuracy / len(test_loader)\n",
    "    \n",
    "    # get average loss\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "\n",
    "    # test_focal_loss = test_focal_loss/ len(test_loader)\n",
    "    \n",
    "    # output test loss statistics\n",
    "    print('Test Loss: {:.6f}'.format(test_loss))\n",
    "    # print('Test Focal Loss: {:.6f}'.format(test_focal_loss))\n",
    "    \n",
    "    accuracy_per_class(class_correct, class_total, class_names, accuracy)\n",
    "    \n",
    "    get_metrics(preds_list, target_list)\n",
    "    today = date.today()\n",
    "    \n",
    "    final_pred_list = preds_list\n",
    "    final_target_list = target_list\n",
    "\n",
    "    \n",
    "    return preds_list, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "13affab8-eca2-4a80-973d-882c0b0771b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.504613\n",
      "Test Accuracy of Classes\n",
      "\n",
      "destroyed\t: 85% \t (6/7)\n",
      "major-damage\t: 21% \t (11/51)\n",
      "minor-damage\t: 44% \t (28/63)\n",
      "no-damage\t: 26% \t (25/95)\n",
      "\n",
      "Test Accuracy of Dataset: \t 31% \t (70/216)\n",
      "Accuracy : tensor(0.3241)\n",
      "F1-score :  tensor(0.3501)\n",
      "Precision :  tensor(0.4904)\n",
      "Recall :  tensor(0.3241)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(3),\n",
       "  tensor(1),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(0),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(1),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(1),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(3),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(1),\n",
       "  tensor(0),\n",
       "  tensor(3),\n",
       "  tensor(2),\n",
       "  tensor(1),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(3),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(3),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(3),\n",
       "  tensor(2),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(3),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(2),\n",
       "  tensor(1)],\n",
       " [tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(0),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(1),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3),\n",
       "  tensor(3)])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_eval(vit_model,inference_loader, criterion , class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alivio-tune",
   "language": "python",
   "name": "alivio-tune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
