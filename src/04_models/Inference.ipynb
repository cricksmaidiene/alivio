{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b263eaa2-8daf-439d-841a-eef9fd2d5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "from datetime import date\n",
    "\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os import path, walk, makedirs\n",
    "from skimage.io import imread\n",
    "from shapely import wkt\n",
    "from shapely.geometry import mapping, MultiPolygon, Polygon\n",
    "from cv2 import fillPoly, imwrite\n",
    "from PIL import Image\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "import shapely.wkt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c005a7b-910f-4000-af2e-dba3e09f0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44624c9b-a3eb-48a2-bdea-78b06a0c9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "INFERENCE_ROOT = os.path.join(ROOT_DIR, 'data','xview_building_damage','inference')\n",
    "UPLOADS_IMG = os.path.join(INFERENCE_ROOT, 'upload', 'img')\n",
    "UPLOADS_JSON = os.path.join(INFERENCE_ROOT, 'upload', 'json')\n",
    "POST_PROCESSED =os.path.join(INFERENCE_ROOT, 'postprocesssed')\n",
    "PRE_PROCESSED =os.path.join(INFERENCE_ROOT, 'preprocesssed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68eb6b26-4a81-490f-b5af-f0cada9f6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_imgs = (\n",
    "    [f\"{UPLOADS_IMG}/{f}\" for f in os.listdir(UPLOADS_IMG)]\n",
    ")\n",
    "uploaded_jsons = (\n",
    "    [f\"{UPLOADS_JSON}/{f}\" for f in os.listdir(UPLOADS_JSON)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8611bae-e329-463f-af36-ca3af18cdc2e",
   "metadata": {},
   "source": [
    "### Convert the Uploaded JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c136d9ae-c7a7-43c3-b480-53f9afde4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_json_data: list[dict] = []\n",
    "\n",
    "def read_and_store_label_json(label_json_path: str):\n",
    "    \"\"\"A thread-safe function that reads a json as a dictionary and writes to a global list\"\"\"\n",
    "    with open(label_json_path) as f:\n",
    "        label_json_data.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be8b2d0b-3c93-4b68-9cbd-b695154319eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_store_label_json(uploaded_jsons[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a3225eb-2ac7-42b0-ba6c-9ccb8ebe9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_json_series: pd.Series = pd.Series(label_json_data)\n",
    "label_df_original: pd.DataFrame = pd.json_normalize(label_json_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de497405-c9e5-4a36-ab09-0c1355b979b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lbl_df: pd.DataFrame = label_df_original.copy()\n",
    "CHALLENGE_TYPE: Literal[\"train\", \"test\", \"hold\"] = \"test\"\n",
    "\n",
    "def json_df_to_csv(label_df):\n",
    "    \n",
    "    label_df_lng_lat: pd.DataFrame = (\n",
    "        label_df.drop(columns=[\"features.xy\", \"features.lng_lat\"])\n",
    "        .join(label_df[\"features.lng_lat\"].explode())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    label_df_features: pd.DataFrame = (\n",
    "        label_df.drop(columns=[\"features.xy\", \"features.lng_lat\"])\n",
    "        .join(label_df[\"features.xy\"].explode())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    lng_lat_normalized: pd.DataFrame = pd.json_normalize(label_df_lng_lat[\"features.lng_lat\"]).rename(\n",
    "        columns={\n",
    "            \"wkt\": \"map_polygon\",\n",
    "            \"properties.feature_type\": \"map_feature_type\",\n",
    "            \"properties.subtype\": \"map_damage\",\n",
    "            \"properties.uid\": \"building_id\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    features_normalized: pd.DataFrame = pd.json_normalize(\n",
    "        label_df_features[\"features.xy\"]\n",
    "    ).rename(\n",
    "        columns={\n",
    "            \"wkt\": \"image_polygon\",\n",
    "            \"properties.feature_type\": \"image_feature_type\",\n",
    "            \"properties.subtype\": \"image_damage\",\n",
    "            \"properties.uid\": \"building_id\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    label_df_lng_lat_normalized = label_df_lng_lat.drop(columns=[\"features.lng_lat\"]).join(\n",
    "        lng_lat_normalized\n",
    "    )\n",
    "    \n",
    "    label_df_features_normalized = label_df_features.drop(columns=[\"features.xy\"]).join(\n",
    "        features_normalized\n",
    "    )\n",
    "    \n",
    "    label_df_final: pd.DataFrame = label_df_lng_lat_normalized.merge(\n",
    "        label_df_features_normalized[\n",
    "            [\n",
    "                \"metadata.id\",\n",
    "                \"image_polygon\",\n",
    "                \"image_feature_type\",\n",
    "                \"image_damage\",\n",
    "                \"building_id\",\n",
    "            ]\n",
    "        ],\n",
    "        \"left\",\n",
    "        [\"metadata.id\", \"building_id\"],\n",
    "    )\n",
    "    \n",
    "    label_df_final = (\n",
    "        label_df_final.rename(\n",
    "            columns={\n",
    "                c: c.replace(\"metadata.\", \"\")\n",
    "                for c in label_df_final.columns\n",
    "                if c.startswith(\"metadata.\")\n",
    "            }\n",
    "        )\n",
    "        .drop(\n",
    "            columns=[\n",
    "                \"map_feature_type\",\n",
    "                \"map_damage\",\n",
    "            ]\n",
    "        )\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"image_feature_type\": \"feature_type\",\n",
    "                \"image_damage\": \"damage\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    label_df_final[\"dataset\"] = CHALLENGE_TYPE\n",
    "    label_df_final[\"capture_date\"] = pd.to_datetime(label_df_final[\"capture_date\"])\n",
    "    \n",
    "    label_df_final[\"image_id\"] = label_df_final[\"img_name\"].dropna().apply(lambda cell: \"_\".join(cell.split(\"_\")[0:2]))\n",
    "    label_df_final[\"is_pre_image\"] = label_df_final[\"img_name\"].dropna().apply(lambda cell: \"_pre_disaster\" in cell)\n",
    "    label_df_final[\"is_post_image\"] = (\n",
    "        label_df_final[\"img_name\"].dropna().apply(lambda cell: \"_post_disaster\" in cell)\n",
    "    )\n",
    "    \n",
    "    label_df_final.to_parquet(f\"{CHALLENGE_TYPE}.parquet\")\n",
    "    \n",
    "    concat_list: list[pd.DataFrame] = [\n",
    "        pd.read_parquet(pq_file) for pq_file in os.listdir() if pq_file.endswith(\".parquet\")\n",
    "    ]\n",
    "    \n",
    "    df = pd.concat(concat_list).reset_index(drop=True)\n",
    "    df.to_parquet(os.path.join(POST_PROCESSED, 'inference_data.parquet'))\n",
    "    \n",
    "    df.to_csv(\n",
    "       os.path.join(POST_PROCESSED, 'inference_data.csv'), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bd256d4-9e61-4547-869e-60a5515cb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_to_csv(lbl_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f3e4b-51c7-4c03-8db9-6fd2d953c936",
   "metadata": {},
   "source": [
    "### Step 2 : Begin Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbcc570e-2f6d-4050-8409-cc66b599d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_class_numeric_labels(df_name):\n",
    "    # df_name['damage'].fillna('pre', inplace=True)\n",
    "    df_name['damage_class']=df_name['damage']\n",
    "    keys=list(df_name['damage_class'].value_counts().keys())\n",
    "    df_name['damage_class']=df_name['damage_class'].apply(keys.index)\n",
    "    df_name['damage_class'].value_counts()\n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ff4faa0-7b5a-4286-9f57-c8f62a8543ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    infer_csv = pd.read_csv(os.path.join(POST_PROCESSED,'inference_data.csv'))\n",
    "    data = infer_csv[infer_csv['image_polygon'].notna()]\n",
    "    df_disaster = data[data['damage'] != 'un-classified']\n",
    "    df_disaster['mask_file_names'] = df_disaster['img_name'].str.replace('.png', '_')+df_disaster['building_id']+'.png'\n",
    "    df_disaster_class_labels = get_df_with_class_numeric_labels(df_disaster)\n",
    "    return df_disaster_class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccf5c7fc-6c5b-4182-b744-fda36624259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_n/997tnw7n3tdg9z81cr8j96dc0000gn/T/ipykernel_34107/2942758920.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_disaster['mask_file_names'] = df_disaster['img_name'].str.replace('.png', '_')+df_disaster['building_id']+'.png'\n",
      "/var/folders/_n/997tnw7n3tdg9z81cr8j96dc0000gn/T/ipykernel_34107/1265940060.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_name['damage_class']=df_name['damage']\n",
      "/var/folders/_n/997tnw7n3tdg9z81cr8j96dc0000gn/T/ipykernel_34107/1265940060.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_name['damage_class']=df_name['damage_class'].apply(keys.index)\n"
     ]
    }
   ],
   "source": [
    "df=get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9316bfa4-a065-4a28-8b9b-c1d4e0f1ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygons_mask(polygons):\n",
    "    img_mask = np.zeros(im_size, np.uint8)\n",
    "    if not polygons:\n",
    "        return img_mask\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "    exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
    "    interiors = [int_coords(pi.coords) for poly in polygons\n",
    "                 for pi in poly.interiors]\n",
    "    cv2.fillPoly(img_mask, exteriors, 1)\n",
    "    cv2.fillPoly(img_mask, interiors, 0)\n",
    "    return img_mask\n",
    "\n",
    "def create_image_mask_overall(root_dir, im_size, meta_df):\n",
    "    input_dir =  os.path.join(root_dir, 'upload') \n",
    "    dest_dir = os.path.join(root_dir, str(date.today()))\n",
    "    img_input = os.path.join(input_dir, 'img')\n",
    "    \n",
    "    if os.path.exists(dest_dir):\n",
    "      print(\"Removing the dir with name: \", dest_dir) \n",
    "      os.system(\"rm -rf \"+dest_dir)\n",
    "        \n",
    "    print(\"creating empty dir with name \" , dest_dir)\n",
    "    os.makedirs(dest_dir)\n",
    "    \n",
    "    img_overlay = os.path.join(dest_dir, 'img_mask_overlay')\n",
    "    if os.path.exists(img_overlay):\n",
    "        print(\"Removing the dir with name: \", img_overlay) \n",
    "        os.system(\"rm -rf \"+img_overlay)\n",
    "    \n",
    "    print(\"creating empty dir with name \" , img_overlay)\n",
    "    os.makedirs(img_overlay)\n",
    "   \n",
    "   \n",
    "   #output_dir =os.path.join(root_dir, 'challenge', dataSplit, 'disaster','hurricanes-all', 'img_mask_overlay', hurricane_name )\n",
    "\n",
    "    df = meta_df[meta_df['is_post_image'] == True]\n",
    "    \n",
    "    print(\"Starting : Mask overlay\")\n",
    "    for idx, file_name in enumerate(df['mask_file_names']):\n",
    "       image = cv2.imread(os.path.join(img_input, df.iloc[idx]['img_name']))\n",
    "       mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "       _mask = polygons_mask([shapely.wkt.loads(df.iloc[idx]['image_polygon'])])\n",
    "       masked = cv2.bitwise_and(image, image, mask=_mask)\n",
    "       plt.imsave(os.path.join(img_overlay, file_name), masked)\n",
    "    print(\"Ending : Mask overlay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2115d5e6-58fa-494c-9b64-ddac0907883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds_tp(image_wkt):\n",
    "    bounds = wkt.loads(image_wkt).bounds\n",
    "    return (bounds[0], bounds[1], bounds[2], bounds[3]) ## \n",
    "\n",
    "def crop_save_masked_images(root_dir, meta_df, crop_output_dir_name = 'img_mask_overlay_crops'):\n",
    "    input_dir =  os.path.join(root_dir, str(date.today())) \n",
    "    img_crop_overlay = os.path.join(input_dir, crop_output_dir_name)\n",
    "    if os.path.exists(img_crop_overlay):\n",
    "        print(\"Removing the dir with name: \", img_crop_overlay) \n",
    "    os.system(\"rm -rf \"+img_crop_overlay)\n",
    "    \n",
    "    print(\"creating empty dir with name \" , img_crop_overlay)\n",
    "    os.makedirs(img_crop_overlay)\n",
    "    \n",
    "    print(\"Starting Cropping the images\")\n",
    "    for idx, file_name in enumerate(meta_df['mask_file_names']):\n",
    "        img = Image.open(os.path.join(input_dir,'img_mask_overlay', file_name))\n",
    "        minx, miny, maxx, maxy = get_bounds_tp(meta_df.iloc[idx]['image_polygon'])\n",
    "        cropped_img=img.crop((minx-5, miny-5, maxx+5, maxy+5))\n",
    "        cropped_img.save(os.path.join(img_crop_overlay, file_name))\n",
    "    print(\"Finished Cropping the images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "674b9458-6cfa-4ede-bebc-1fe4ddbfdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_masks_by_class(top_dir, meta_df, cls_path='img_mask_ov_crop_class'):\n",
    "    input_dir =  os.path.join(top_dir, str(date.today())) \n",
    "    disas_post_mask= os.path.join(input_dir, 'img_mask_overlay_crops') #source\n",
    "    print(\"Source root : \", disas_post_mask)\n",
    "    disas_class_path=os.path.join(input_dir, 'img_mask_ov_crop_class' )\n",
    "    if os.path.exists(disas_class_path):\n",
    "        print(\"Removing the dir with name: \", disas_class_path) \n",
    "    os.system(\"rm -rf \"+disas_class_path)\n",
    "    \n",
    "    print(\"creating empty dir with name \" , disas_class_path)\n",
    "    os.makedirs(disas_class_path)\n",
    "    \n",
    "    print(\"Destination root : \", disas_class_path)\n",
    "    \n",
    "    df = meta_df[meta_df['is_post_image'] == True]\n",
    "    \n",
    "    print(\"Started moving the mask files to class folder \")\n",
    "    for idx, file_name in enumerate(df['mask_file_names']):\n",
    "        source = os.path.join(disas_post_mask, df.iloc[idx]['mask_file_names'])\n",
    "        destination = os.path.join(disas_class_path, df.iloc[idx]['damage'])\n",
    "        if os.path.exists(destination):\n",
    "            pass\n",
    "        else:\n",
    "            print( \"Creating dir for \" , df.iloc[idx]['damage'])\n",
    "            os.makedirs(destination)\n",
    "        \n",
    "        if os.path.exists(source):\n",
    "            shutil.copy(source, destination)\n",
    "    print(\"Finshed moving the mask files to class folder \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1636bd1-9769-4c02-b7c8-47916bf1eed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the dir with name:  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02\n",
      "creating empty dir with name  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02\n",
      "creating empty dir with name  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_overlay\n",
      "Starting : Mask overlay\n",
      "Ending : Mask overlay\n"
     ]
    }
   ],
   "source": [
    "im_size =(1024, 1024)\n",
    "create_image_mask_overall(INFERENCE_ROOT, im_size, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e6c604e-1dd2-4ba7-8c23-34ea273ceffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the dir with name:  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_overlay_crops\n",
      "creating empty dir with name  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_overlay_crops\n",
      "Starting Cropping the images\n",
      "Finished Cropping the images\n"
     ]
    }
   ],
   "source": [
    "crop_save_masked_images(INFERENCE_ROOT, df, crop_output_dir_name = 'img_mask_overlay_crops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e54eb3b5-87f9-4cb8-9ed5-0d0de22089d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_overlay_crops\n",
      "creating empty dir with name  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_ov_crop_class\n",
      "Destination root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/cleaned_repo/alivio/data/xview_building_damage/inference/2024-04-02/img_mask_ov_crop_class\n",
      "Started moving the mask files to class folder \n",
      "Creating dir for  no-damage\n",
      "Creating dir for  minor-damage\n",
      "Creating dir for  major-damage\n",
      "Creating dir for  destroyed\n",
      "Finshed moving the mask files to class folder \n"
     ]
    }
   ],
   "source": [
    "sort_masks_by_class(INFERENCE_ROOT , df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alivio-tune",
   "language": "python",
   "name": "alivio-tune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
