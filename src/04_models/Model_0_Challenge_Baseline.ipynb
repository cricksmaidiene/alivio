{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cec042c-0bcf-4713-8ff4-7a9b72fae8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import ViTModel, ViTImageProcessor, ViTForImageClassification\n",
    "from timm import create_model\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shapely.wkt\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import ast\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Add, Input, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras import backend as K\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "586d7881-0a76-4b39-a6a9-e4a41071857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('../..')\n",
    "root_dir=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de4a497f-3743-466f-a499-fc5ea28f2332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Project Path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio\n",
      "Root Data Path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage\n",
      "Train Data Path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/train\n"
     ]
    }
   ],
   "source": [
    "print(\"Root Project Path : \", root_dir)\n",
    "WEIGHT_DIR = os.path.join(root_dir, 'weights')\n",
    "ROOT_DATA_DIR = os.path.join(root_dir, 'data', 'xview_building_damage')\n",
    "MODEL_OUT = os.path.join(ROOT_DATA_DIR, 'model')\n",
    "print(\"Root Data Path : \", ROOT_DATA_DIR)\n",
    "TRAIN_DATA_DIR = os.path.join(ROOT_DATA_DIR, 'train')\n",
    "print(\"Train Data Path : \", TRAIN_DATA_DIR)\n",
    "CHALLENGE_DIR = os.path.join(ROOT_DATA_DIR, 'challenge')\n",
    "TRAIN_DIR =  os.path.join(CHALLENGE_DIR, 'train')\n",
    "\n",
    "TRAIN_DAT_DIR= os.path.join(TRAIN_DIR, 'disaster', 'hurricane-michael')\n",
    "TEST_DIR =  os.path.join(CHALLENGE_DIR, 'test')\n",
    "\n",
    "Test_disaster= os.path.join(TEST_DIR, 'disaster', 'hurricane-michael' )\n",
    "TEST_DAT_DIR= os.path.join(TEST_DIR, 'disaster', 'hurricane-michael')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4abe3cdf-8a93-43b2-9969-9afcae4835a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(root_dir, 'logs')\n",
    "NUM_WORKERS = 4 \n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100 \n",
    "LEARNING_RATE = 0.0001\n",
    "RANDOM_SEED = 123\n",
    "LOG_STEP = 150\n",
    "LOG_DIR = LOG_DIR + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada72d7a-ebeb-4759-a3c8-fd49a20e05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_intensity_encoding = dict()\n",
    "damage_intensity_encoding[5] = '5'\n",
    "damage_intensity_encoding[4] = '4'\n",
    "damage_intensity_encoding[3] = '3'\n",
    "damage_intensity_encoding[2] = '2' \n",
    "damage_intensity_encoding[1] = '1' \n",
    "damage_intensity_encoding[0] = '0' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99ce479b-5c7e-4f3a-a85e-527ec35ef0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DIR : /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/csv/hc_train_labels_hurricane-michael.csv\n",
      "Train DIR : /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/csv/hc_test_labels_hurricane-michael.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'sensor', 'provider_asset_type', 'gsd', 'capture_date',\n",
       "       'off_nadir_angle', 'pan_resolution', 'sun_azimuth', 'sun_elevation',\n",
       "       'target_azimuth', 'disaster', 'disaster_type', 'catalog_id',\n",
       "       'original_width', 'original_height', 'width', 'height', 'id',\n",
       "       'img_name', 'map_polygon', 'building_id', 'image_polygon',\n",
       "       'feature_type', 'damage', 'dataset', 'image_id', 'is_pre_image',\n",
       "       'is_post_image', 'mask_file_names', 'damage_class', 'labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = os.path.join(CHALLENGE_DIR, 'csv', 'hc_train_hurricane-michael.csv')\n",
    "test_csv = os.path.join(CHALLENGE_DIR, 'csv','hc_test_hurricane-michael.csv')\n",
    "\n",
    "df_test=pd.read_csv(test_csv)\n",
    "df_test['labels'] = df_test['damage']\n",
    "df_test['labels'].value_counts()\n",
    "df_test.to_csv(os.path.join(CHALLENGE_DIR, 'csv','hc_test_labels_hurricane-michael.csv'))\n",
    "test_csv_labels=os.path.join(CHALLENGE_DIR, 'csv','hc_test_labels_hurricane-michael.csv')\n",
    "\n",
    "df_train=pd.read_csv(train_csv)\n",
    "df_train['labels'] = df_train['damage']\n",
    "df_train['labels'].value_counts()\n",
    "df_train.to_csv(os.path.join(CHALLENGE_DIR, 'csv','hc_train_labels_hurricane-michael.csv'))\n",
    "train_csv_labels=os.path.join(CHALLENGE_DIR, 'csv','hc_train_labels_hurricane-michael.csv')\n",
    "\n",
    "print(\"Train DIR :\", train_csv_labels) \n",
    "print(\"Train DIR :\", test_csv_labels) \n",
    "\n",
    "class_weights=compute_class_weight(class_weight = 'balanced',classes= np.unique(df_train['labels'].to_list()), y = df_train['labels'].to_list())\n",
    "d_class_weights = dict(enumerate(class_weights))\n",
    "d_class_weights\n",
    "\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e424fc-9217-4698-95d1-36b011604dcd",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5755ad-722b-4b83-973d-de44bd0d22cb",
   "metadata": {},
   "source": [
    "This code is adapted from XView2 Challenge Baseline https://github.com/DIUx-xView/xView2_baseline/blob/master/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da68af1-b582-4134-a534-8d59b90eea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "###\n",
    "# Creates data generator for validation set\n",
    "###\n",
    "def validation_generator(test_csv, test_dir):\n",
    "    df = pd.read_csv(test_csv)\n",
    "    df = df.replace({\"labels\" : damage_intensity_encoding })\n",
    "\n",
    "    gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "                             rescale=1/255.)\n",
    "\n",
    "\n",
    "    return gen.flow_from_dataframe(dataframe=df,\n",
    "                                   directory=test_dir,\n",
    "                                   x_col='building_id',\n",
    "                                   y_col='labels',\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   shuffle=False,\n",
    "                                   seed=RANDOM_SEED,\n",
    "                                   class_mode=\"categorical\",\n",
    "                                   target_size=(128, 128))\n",
    "\n",
    "\n",
    "###\n",
    "# Applies random transformations to training data\n",
    "###\n",
    "def augment_data(df, in_dir):\n",
    "\n",
    "    df = df.replace({\"labels\" : damage_intensity_encoding })\n",
    "    data_gen_train = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,\n",
    "                             vertical_flip=True,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             rescale=1/255.)\n",
    "    train_generator = data_gen_train.flow_from_directory(in_dir, target_size=(128,128), batch_size=128, class_mode=\"categorical\")\n",
    "    return train_generator\n",
    "    # return gen.flow_from_dataframe(dataframe=df,\n",
    "    #                                directory=in_dir,\n",
    "    #                                x_col='building_id',\n",
    "    #                                y_col='labels',\n",
    "    #                               # batch_size=BATCH_SIZE,\n",
    "    #                                seed=RANDOM_SEED,\n",
    "    #                                class_mode=\"categorical\",\n",
    "    #                                target_size=(128, 128))\n",
    "\n",
    "\n",
    "# Run training and evaluation based on existing or new model\n",
    "def train_model(train_data, train_csv, test_data, test_csv, model_out, model_in = None):\n",
    "\n",
    "    model = generate_xBD_baseline_model()\n",
    "\n",
    "    # Add model weights if provided by user\n",
    "    if model_in is not None:\n",
    "        model.load_weights(model_in)\n",
    "\n",
    "    df = pd.read_csv(train_csv)\n",
    "    class_weights=compute_class_weight(class_weight = 'balanced',classes= np.unique(df_train['labels'].to_list()), y = df_train['labels'].to_list())\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    samples = df['building_id'].count()\n",
    "    steps = np.ceil(samples/BATCH_SIZE)\n",
    "\n",
    "    # Augments the training data\n",
    "    train_gen_flow = augment_data(df, train_data)\n",
    "\n",
    "    #Set up tensorboard logging\n",
    "    tensorboard_callbacks = keras.callbacks.TensorBoard(log_dir=LOG_DIR,\n",
    "                                                        batch_size=BATCH_SIZE)\n",
    "\n",
    "    \n",
    "    #Filepath to save model weights\n",
    "    filepath = model_out + \"-saved-model-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
    "    checkpoints = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                    monitor=['loss', 'accuracy'],\n",
    "                                                    verbose=1,\n",
    "                                                    save_best_only=False,\n",
    "                                                    mode='max')\n",
    "\n",
    "    #Adds adam optimizer\n",
    "    adam = keras.optimizers.Adam(lr=LEARNING_RATE,\n",
    "                                    beta_1=0.9,\n",
    "                                    beta_2=0.999,\n",
    "                                    weight_decay=0.0,\n",
    "                                    amsgrad=False)\n",
    "\n",
    "\n",
    "    model.compile(loss=ordinal_loss, optimizer=adam, metrics=['accuracy', f1])\n",
    "\n",
    "    #Training begins\n",
    "    model.fit(train_gen_flow,\n",
    "                steps_per_epoch=steps,\n",
    "                epochs=NUM_EPOCHS,\n",
    "                workers=NUM_WORKERS,\n",
    "                #use_multiprocessing=True,\n",
    "                class_weight=d_class_weights,\n",
    "                #callbacks=[tensorboard_callbacks, checkpoints],\n",
    "                verbose=1)\n",
    "\n",
    "\n",
    "    #Evalulate f1 weighted scores on validation set\n",
    "    validation_gen = validation_generator(test_csv, test_data)\n",
    "    predictions = model.predict(validation_gen)\n",
    "\n",
    "    val_trues = validation_gen.classes\n",
    "    val_pred = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    f1_weighted = f1_score(val_trues, val_pred, average='weighted')\n",
    "    print(f1_weighted)\n",
    "\n",
    "def ordinal_loss(y_true, y_pred):\n",
    "    weights = K.cast(K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n",
    "    return (1.0 + weights) * keras.losses.categorical_crossentropy(y_true, y_pred )\n",
    "\n",
    "def generate_xBD_baseline_model():\n",
    "  weights = 'imagenet'\n",
    "  inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "  base_model = ResNet50(include_top=False, weights=weights, input_shape=(128, 128, 3))\n",
    "\n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "  x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', activation='relu', input_shape=(128, 128, 3))(inputs)\n",
    "  x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "  x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "  x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "  x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "  x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "  x = Flatten()(x)\n",
    "\n",
    "  base_resnet = base_model(inputs)\n",
    "  base_resnet = Flatten()(base_resnet)\n",
    "\n",
    "  concated_layers = Concatenate()([x, base_resnet])\n",
    "\n",
    "  concated_layers = Dense(2024, activation='relu')(concated_layers)\n",
    "  concated_layers = Dense(524, activation='relu')(concated_layers)\n",
    "  concated_layers = Dense(124, activation='relu')(concated_layers)\n",
    "  output = Dense(4, activation='relu')(concated_layers)\n",
    "\n",
    "  model = Model(inputs=inputs, outputs=output)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f72d78-b899-4155-a4ff-85c2e3a5bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Run Building Damage Classification Training & Evaluation')\n",
    "parser.add_argument('--train_data',\n",
    "                    required=True,\n",
    "                    metavar=\"/path/to/xBD_train\",\n",
    "                    help=\"Full path to the train data directory\")\n",
    "parser.add_argument('--train_csv',\n",
    "                    required=True,\n",
    "                    metavar=\"/path/to/xBD_split\",\n",
    "                    help=\"Full path to the train csv\")\n",
    "parser.add_argument('--test_data',\n",
    "                    required=True,\n",
    "                    metavar=\"/path/to/xBD_test\",\n",
    "                    help=\"Full path to the test data directory\")\n",
    "parser.add_argument('--test_csv',\n",
    "                    required=True,\n",
    "                    metavar=\"/path/to/xBD_split\",\n",
    "                    help=\"Full path to the test csv\")\n",
    "parser.add_argument('--model_in',\n",
    "                    default=None,\n",
    "                    metavar='/path/to/input_model',\n",
    "                    help=\"Path to save model\")\n",
    "parser.add_argument('--model_out',\n",
    "                    required=True,\n",
    "                    metavar='/path/to/save_model',\n",
    "                    help=\"Path to save model\")\n",
    "\n",
    "args = parser.parse_args(['--train_data', TRAIN_DAT_DIR , \\\n",
    "                          '--train_csv', train_csv_labels, \\\n",
    "                          '--test_data', TEST_DAT_DIR, \\\n",
    "                          '--test_csv', test_csv_labels, \\\n",
    "                          '--model_out', MODEL_OUT])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9d27a-2635-4b12-9127-270ca9e48f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 136802 images belonging to 4 classes.\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "709/709 [==============================] - 1132s 2s/step - loss: 107.3141 - accuracy: 0.6624 - f1: 0.6618\n",
      "Epoch 2/100\n",
      "709/709 [==============================] - 1135s 2s/step - loss: 107.3675 - accuracy: 0.6632 - f1: 0.6632\n",
      "Epoch 3/100\n",
      "609/709 [========================>.....] - ETA: 3:02 - loss: 107.0224 - accuracy: 0.6643 - f1: 0.6643"
     ]
    }
   ],
   "source": [
    "train_model(args.train_data, args.train_csv, args.test_data, args.test_csv ,args.model_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
