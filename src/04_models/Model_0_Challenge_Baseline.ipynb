{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cec042c-0bcf-4713-8ff4-7a9b72fae8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import ViTModel, ViTImageProcessor, ViTForImageClassification\n",
    "from timm import create_model\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shapely.wkt\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import ast\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Add, Input, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras import backend as K\n",
    "import platform\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8427e9-de03-4026-93d7-68fa01df508e",
   "metadata": {},
   "source": [
    "### Steps to install tensor-metal plugin \n",
    "- I followed this https://www.linkedin.com/pulse/how-use-gpu-tensorflow-pytorch-libraries-macbook-pro-m2apple-kashyap/\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba14dd3-4113-4509-85bd-3f8615197d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3acf62-e6b7-4aba-9755-f664131ed0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-14.3.1-arm64-arm-64bit\n",
      "Tensor Flow Version: 2.15.0\n",
      "\n",
      "Python 3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ]\n",
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "GPU is NOT AVAILABLE\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "#gpu_available = tf.test.is_gpu_available()\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "#print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "#tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586d7881-0a76-4b39-a6a9-e4a41071857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"../..\")\n",
    "root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4a497f-3743-466f-a499-fc5ea28f2332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Project Path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio\n",
      "Root Data Path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage\n",
      "Train Data Path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/train\n"
     ]
    }
   ],
   "source": [
    "print(\"Root Project Path : \", root_dir)\n",
    "WEIGHT_DIR = os.path.join(root_dir, \"weights\")\n",
    "ROOT_DATA_DIR = os.path.join(root_dir, \"data\", \"xview_building_damage\")\n",
    "MODEL_OUT = os.path.join(ROOT_DATA_DIR, \"model\")\n",
    "print(\"Root Data Path : \", ROOT_DATA_DIR)\n",
    "TRAIN_DATA_DIR = os.path.join(ROOT_DATA_DIR, \"train\")\n",
    "print(\"Train Data Path : \", TRAIN_DATA_DIR)\n",
    "CHALLENGE_DIR = os.path.join(ROOT_DATA_DIR, \"challenge\")\n",
    "TRAIN_DIR = os.path.join(CHALLENGE_DIR, \"train\")\n",
    "\n",
    "TRAIN_DAT_DIR = os.path.join(TRAIN_DIR, \"disaster\", \"hurricane-michael\")\n",
    "TEST_DIR = os.path.join(CHALLENGE_DIR, \"test\")\n",
    "\n",
    "Test_disaster = os.path.join(TEST_DIR, \"disaster\", \"hurricane-michael\")\n",
    "TEST_DAT_DIR = os.path.join(TEST_DIR, \"disaster\", \"hurricane-michael\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4abe3cdf-8a93-43b2-9969-9afcae4835a9",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "LOG_DIR = os.path.join(root_dir, 'logs')\n",
    "NUM_WORKERS = 4 \n",
    "NUM_CLASSES = 6\n",
    "BATCH_SIZE = 64\n",
    "#NUM_EPOCHS = 100 \n",
    "NUM_EPOCHS = 50 \n",
=======
    "LOG_DIR = os.path.join(root_dir, \"logs\")\n",
    "NUM_WORKERS = 4\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
>>>>>>> main
    "LEARNING_RATE = 0.0001\n",
    "RANDOM_SEED = 123\n",
    "LOG_STEP = 150\n",
    "LOG_DIR = LOG_DIR + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada72d7a-ebeb-4759-a3c8-fd49a20e05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_intensity_encoding = dict()\n",
    "damage_intensity_encoding[5] = \"5\"\n",
    "damage_intensity_encoding[4] = \"4\"\n",
    "damage_intensity_encoding[3] = \"3\"\n",
    "damage_intensity_encoding[2] = \"2\"\n",
    "damage_intensity_encoding[1] = \"1\"\n",
    "damage_intensity_encoding[0] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99ce479b-5c7e-4f3a-a85e-527ec35ef0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DIR : /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/csv/hc_train_labels_hurricane-michael.csv\n",
      "Train DIR : /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/csv/hc_test_labels_hurricane-michael.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'sensor', 'provider_asset_type', 'gsd', 'capture_date',\n",
       "       'off_nadir_angle', 'pan_resolution', 'sun_azimuth', 'sun_elevation',\n",
       "       'target_azimuth', 'disaster', 'disaster_type', 'catalog_id',\n",
       "       'original_width', 'original_height', 'width', 'height', 'id',\n",
       "       'img_name', 'map_polygon', 'building_id', 'image_polygon',\n",
       "       'feature_type', 'damage', 'dataset', 'image_id', 'is_pre_image',\n",
       "       'is_post_image', 'mask_file_names', 'damage_class', 'labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = os.path.join(CHALLENGE_DIR, \"csv\", \"hc_train_hurricane-michael.csv\")\n",
    "test_csv = os.path.join(CHALLENGE_DIR, \"csv\", \"hc_test_hurricane-michael.csv\")\n",
    "\n",
    "df_test = pd.read_csv(test_csv)\n",
    "df_test[\"labels\"] = df_test[\"damage\"]\n",
    "df_test[\"labels\"].value_counts()\n",
    "df_test.to_csv(\n",
    "    os.path.join(CHALLENGE_DIR, \"csv\", \"hc_test_labels_hurricane-michael.csv\")\n",
    ")\n",
    "test_csv_labels = os.path.join(\n",
    "    CHALLENGE_DIR, \"csv\", \"hc_test_labels_hurricane-michael.csv\"\n",
    ")\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_train[\"labels\"] = df_train[\"damage\"]\n",
    "df_train[\"labels\"].value_counts()\n",
    "df_train.to_csv(\n",
    "    os.path.join(CHALLENGE_DIR, \"csv\", \"hc_train_labels_hurricane-michael.csv\")\n",
    ")\n",
    "train_csv_labels = os.path.join(\n",
    "    CHALLENGE_DIR, \"csv\", \"hc_train_labels_hurricane-michael.csv\"\n",
    ")\n",
    "\n",
    "print(\"Train DIR :\", train_csv_labels)\n",
    "print(\"Train DIR :\", test_csv_labels)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(df_train[\"labels\"].to_list()),\n",
    "    y=df_train[\"labels\"].to_list(),\n",
    ")\n",
    "d_class_weights = dict(enumerate(class_weights))\n",
    "d_class_weights\n",
    "\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e424fc-9217-4698-95d1-36b011604dcd",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5755ad-722b-4b83-973d-de44bd0d22cb",
   "metadata": {},
   "source": [
    "This code is adapted from XView2 Challenge Baseline https://github.com/DIUx-xView/xView2_baseline/blob/master/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da68af1-b582-4134-a534-8d59b90eea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "###\n",
    "# Creates data generator for validation set\n",
    "###\n",
    "def validation_generator(test_csv, test_dir):\n",
    "    df = pd.read_csv(test_csv)\n",
    "    df = df.replace({\"labels\": damage_intensity_encoding})\n",
    "\n",
    "    gen = keras.preprocessing.image.ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "    return gen.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        directory=test_dir,\n",
    "        x_col=\"building_id\",\n",
    "        y_col=\"labels\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        seed=RANDOM_SEED,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(128, 128),\n",
    "    )\n",
    "\n",
    "\n",
    "###\n",
    "# Applies random transformations to training data\n",
    "###\n",
    "def augment_data(df, in_dir):\n",
    "\n",
    "    df = df.replace({\"labels\": damage_intensity_encoding})\n",
    "    data_gen_train = keras.preprocessing.image.ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        rescale=1 / 255.0,\n",
    "    )\n",
    "    train_generator = data_gen_train.flow_from_directory(\n",
    "        in_dir, target_size=(128, 128), batch_size=128, class_mode=\"categorical\"\n",
    "    )\n",
    "    return train_generator\n",
    "    # return gen.flow_from_dataframe(dataframe=df,\n",
    "    #                                directory=in_dir,\n",
    "    #                                x_col='building_id',\n",
    "    #                                y_col='labels',\n",
    "    #                               # batch_size=BATCH_SIZE,\n",
    "    #                                seed=RANDOM_SEED,\n",
    "    #                                class_mode=\"categorical\",\n",
    "    #                                target_size=(128, 128))\n",
    "\n",
    "\n",
    "# Run training and evaluation based on existing or new model\n",
    "def train_model(train_data, train_csv, test_data, test_csv, model_out, model_in=None):\n",
    "\n",
    "    model = generate_xBD_baseline_model()\n",
    "\n",
    "    # Add model weights if provided by user\n",
    "    if model_in is not None:\n",
    "        model.load_weights(model_in)\n",
    "\n",
    "    df = pd.read_csv(train_csv)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(df_train[\"labels\"].to_list()),\n",
    "        y=df_train[\"labels\"].to_list(),\n",
    "    )\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    samples = df[\"building_id\"].count()\n",
    "    steps = np.ceil(samples / BATCH_SIZE)\n",
    "\n",
    "    # Augments the training data\n",
    "    train_gen_flow = augment_data(df, train_data)\n",
    "\n",
    "    # Set up tensorboard logging\n",
    "    tensorboard_callbacks = keras.callbacks.TensorBoard(\n",
    "        log_dir=LOG_DIR, batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Filepath to save model weights\n",
    "    filepath = model_out + \"-saved-model-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
    "    checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor=[\"loss\", \"accuracy\"],\n",
    "        verbose=1,\n",
    "        save_best_only=False,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "\n",
    "    # Adds adam optimizer\n",
    "    adam = keras.optimizers.Adam(\n",
    "        lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, weight_decay=0.0, amsgrad=False\n",
    "    )\n",
    "\n",
    "    model.compile(loss=ordinal_loss, optimizer=adam, metrics=[\"accuracy\", f1])\n",
    "\n",
    "    # Training begins\n",
    "    model.fit(\n",
    "        train_gen_flow,\n",
    "        steps_per_epoch=steps,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        workers=NUM_WORKERS,\n",
    "        # use_multiprocessing=True,\n",
    "        class_weight=d_class_weights,\n",
    "        # callbacks=[tensorboard_callbacks, checkpoints],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Evalulate f1 weighted scores on validation set\n",
    "    validation_gen = validation_generator(test_csv, test_data)\n",
    "    predictions = model.predict(validation_gen)\n",
    "\n",
    "    val_trues = validation_gen.classes\n",
    "    val_pred = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    f1_weighted = f1_score(val_trues, val_pred, average=\"weighted\")\n",
    "    print(f1_weighted)\n",
    "\n",
    "\n",
    "def ordinal_loss(y_true, y_pred):\n",
    "    weights = K.cast(\n",
    "        K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))\n",
    "        / (K.int_shape(y_pred)[1] - 1),\n",
    "        dtype=\"float32\",\n",
    "    )\n",
    "    return (1.0 + weights) * keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "def generate_xBD_baseline_model():\n",
    "    weights = \"imagenet\"\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "    base_model = ResNet50(include_top=False, weights=weights, input_shape=(128, 128, 3))\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = Conv2D(\n",
    "        32,\n",
    "        (5, 5),\n",
    "        strides=(1, 1),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        input_shape=(128, 128, 3),\n",
    "    )(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", data_format=None)(\n",
    "        x\n",
    "    )\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", data_format=None)(\n",
    "        x\n",
    "    )\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", data_format=None)(\n",
    "        x\n",
    "    )\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    base_resnet = base_model(inputs)\n",
    "    base_resnet = Flatten()(base_resnet)\n",
    "\n",
    "    concated_layers = Concatenate()([x, base_resnet])\n",
    "\n",
    "    concated_layers = Dense(2024, activation=\"relu\")(concated_layers)\n",
    "    concated_layers = Dense(524, activation=\"relu\")(concated_layers)\n",
    "    concated_layers = Dense(124, activation=\"relu\")(concated_layers)\n",
    "    output = Dense(4, activation=\"relu\")(concated_layers)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f72d78-b899-4155-a4ff-85c2e3a5bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Run Building Damage Classification Training & Evaluation\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_data\",\n",
    "    required=True,\n",
    "    metavar=\"/path/to/xBD_train\",\n",
    "    help=\"Full path to the train data directory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_csv\",\n",
    "    required=True,\n",
    "    metavar=\"/path/to/xBD_split\",\n",
    "    help=\"Full path to the train csv\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--test_data\",\n",
    "    required=True,\n",
    "    metavar=\"/path/to/xBD_test\",\n",
    "    help=\"Full path to the test data directory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--test_csv\",\n",
    "    required=True,\n",
    "    metavar=\"/path/to/xBD_split\",\n",
    "    help=\"Full path to the test csv\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_in\",\n",
    "    default=None,\n",
    "    metavar=\"/path/to/input_model\",\n",
    "    help=\"Path to save model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_out\",\n",
    "    required=True,\n",
    "    metavar=\"/path/to/save_model\",\n",
    "    help=\"Path to save model\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\n",
    "    [\n",
    "        \"--train_data\",\n",
    "        TRAIN_DAT_DIR,\n",
    "        \"--train_csv\",\n",
    "        train_csv_labels,\n",
    "        \"--test_data\",\n",
    "        TEST_DAT_DIR,\n",
    "        \"--test_csv\",\n",
    "        test_csv_labels,\n",
    "        \"--model_out\",\n",
    "        MODEL_OUT,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b9d27a-2635-4b12-9127-270ca9e48f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 136802 images belonging to 4 classes.\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "709/709 [==============================] - 997s 1s/step - loss: 2.4309 - accuracy: 0.3315 - f1: 0.6589\n",
      "Epoch 2/50\n",
      "709/709 [==============================] - 6633s 9s/step - loss: 2.2595 - accuracy: 0.3320 - f1: 0.6634\n",
      "Epoch 3/50\n",
      "709/709 [==============================] - 5233s 7s/step - loss: 2.2682 - accuracy: 0.3331 - f1: 0.6633\n",
      "Epoch 4/50\n",
      "709/709 [==============================] - 8218s 12s/step - loss: 2.2711 - accuracy: 0.3313 - f1: 0.6633\n",
      "Epoch 5/50\n",
      "709/709 [==============================] - 6198s 9s/step - loss: 2.2523 - accuracy: 0.3324 - f1: 0.6634\n",
      "Epoch 6/50\n",
      "709/709 [==============================] - 5652s 8s/step - loss: 2.2766 - accuracy: 0.3312 - f1: 0.6632\n",
      "Epoch 7/50\n",
      "709/709 [==============================] - 4977s 7s/step - loss: 2.2766 - accuracy: 0.3304 - f1: 0.6632\n",
      "Epoch 8/50\n",
      "709/709 [==============================] - 2792s 4s/step - loss: 2.2547 - accuracy: 0.3314 - f1: 0.6634\n",
      "Epoch 9/50\n",
      "709/709 [==============================] - 1078s 2s/step - loss: 2.2719 - accuracy: 0.3310 - f1: 0.6633\n",
      "Epoch 10/50\n",
      "709/709 [==============================] - 997s 1s/step - loss: 2.2477 - accuracy: 0.3314 - f1: 0.6635\n",
      "Epoch 11/50\n",
      "709/709 [==============================] - 4762s 7s/step - loss: 2.2893 - accuracy: 0.3325 - f1: 0.6631\n",
      "Epoch 12/50\n",
      "709/709 [==============================] - 5058s 7s/step - loss: 2.2674 - accuracy: 0.3307 - f1: 0.6633\n",
      "Epoch 13/50\n",
      "709/709 [==============================] - 8412s 12s/step - loss: 2.2513 - accuracy: 0.3308 - f1: 0.6634\n",
      "Epoch 14/50\n",
      "709/709 [==============================] - 5817s 8s/step - loss: 2.2619 - accuracy: 0.3310 - f1: 0.6633\n",
      "Epoch 15/50\n",
      "709/709 [==============================] - 6206s 9s/step - loss: 2.2661 - accuracy: 0.3324 - f1: 0.6633\n",
      "Epoch 16/50\n",
      "709/709 [==============================] - 987s 1s/step - loss: 2.2648 - accuracy: 0.3313 - f1: 0.6633\n",
      "Epoch 17/50\n",
      "709/709 [==============================] - 989s 1s/step - loss: 2.2509 - accuracy: 0.3325 - f1: 0.6634\n",
      "Epoch 18/50\n",
      "709/709 [==============================] - 1002s 1s/step - loss: 2.2641 - accuracy: 0.3328 - f1: 0.6633\n",
      "Epoch 19/50\n",
      "709/709 [==============================] - 1013s 1s/step - loss: 2.2641 - accuracy: 0.3317 - f1: 0.6633\n",
      "Epoch 20/50\n",
      "709/709 [==============================] - 6551s 9s/step - loss: 2.2952 - accuracy: 0.3302 - f1: 0.6631\n",
      "Epoch 21/50\n",
      "709/709 [==============================] - 3391s 5s/step - loss: 2.2597 - accuracy: 0.3328 - f1: 0.6634\n",
      "Epoch 22/50\n",
      "709/709 [==============================] - 1026s 1s/step - loss: 2.2573 - accuracy: 0.3333 - f1: 0.6634\n",
      "Epoch 23/50\n",
      "709/709 [==============================] - 4001s 6s/step - loss: 2.2778 - accuracy: 0.3321 - f1: 0.6632\n",
      "Epoch 24/50\n",
      "709/709 [==============================] - 6720s 9s/step - loss: 2.2639 - accuracy: 0.3317 - f1: 0.6633\n",
      "Epoch 25/50\n",
      "709/709 [==============================] - 6816s 10s/step - loss: 2.2656 - accuracy: 0.3325 - f1: 0.6633\n",
      "Epoch 26/50\n",
      "709/709 [==============================] - 7152s 10s/step - loss: 2.2509 - accuracy: 0.3326 - f1: 0.6634\n",
      "Epoch 27/50\n",
      "709/709 [==============================] - 8313s 12s/step - loss: 2.2575 - accuracy: 0.3327 - f1: 0.6634\n",
      "Epoch 28/50\n",
      "709/709 [==============================] - 5424s 8s/step - loss: 2.2822 - accuracy: 0.3304 - f1: 0.6632\n",
      "Epoch 29/50\n",
      "709/709 [==============================] - 5642s 8s/step - loss: 2.2510 - accuracy: 0.3317 - f1: 0.6634\n",
      "Epoch 30/50\n",
      "709/709 [==============================] - 4032s 6s/step - loss: 2.2723 - accuracy: 0.3323 - f1: 0.6633\n",
      "Epoch 31/50\n",
      "709/709 [==============================] - 2076s 3s/step - loss: 2.2498 - accuracy: 0.3321 - f1: 0.6634\n",
      "Epoch 32/50\n",
      "709/709 [==============================] - 8729s 12s/step - loss: 2.2702 - accuracy: 0.3310 - f1: 0.6633\n",
      "Epoch 33/50\n",
      "709/709 [==============================] - 1023s 1s/step - loss: 2.2876 - accuracy: 0.3319 - f1: 0.6631\n",
      "Epoch 34/50\n",
      "709/709 [==============================] - 1024s 1s/step - loss: 2.2647 - accuracy: 0.3314 - f1: 0.6633\n",
      "Epoch 35/50\n",
      "709/709 [==============================] - 1001s 1s/step - loss: 2.2491 - accuracy: 0.3319 - f1: 0.6634\n",
      "Epoch 36/50\n",
      "709/709 [==============================] - 1003s 1s/step - loss: 2.2649 - accuracy: 0.3321 - f1: 0.6633\n",
      "Epoch 37/50\n",
      "709/709 [==============================] - 7790s 11s/step - loss: 2.2591 - accuracy: 0.3303 - f1: 0.6634\n",
      "Epoch 38/50\n",
      "709/709 [==============================] - 5180s 7s/step - loss: 2.2562 - accuracy: 0.3305 - f1: 0.6634\n",
      "Epoch 39/50\n",
      "709/709 [==============================] - 1417s 2s/step - loss: 2.2609 - accuracy: 0.3325 - f1: 0.6634\n",
      "Epoch 40/50\n",
      "709/709 [==============================] - 1030s 1s/step - loss: 2.2599 - accuracy: 0.3310 - f1: 0.6634\n",
      "Epoch 41/50\n",
      "709/709 [==============================] - 3977s 6s/step - loss: 2.2701 - accuracy: 0.3318 - f1: 0.6633\n",
      "Epoch 42/50\n",
      "709/709 [==============================] - 3232s 5s/step - loss: 2.2555 - accuracy: 0.3330 - f1: 0.6634\n",
      "Epoch 43/50\n",
      "709/709 [==============================] - 1000s 1s/step - loss: 2.2739 - accuracy: 0.3321 - f1: 0.6632\n",
      "Epoch 44/50\n",
      "709/709 [==============================] - 1014s 1s/step - loss: 2.2528 - accuracy: 0.3332 - f1: 0.6634\n",
      "Epoch 45/50\n",
      "709/709 [==============================] - 1048s 1s/step - loss: 2.2610 - accuracy: 0.3331 - f1: 0.6634\n",
      "Epoch 46/50\n",
      "709/709 [==============================] - 1136s 2s/step - loss: 2.2758 - accuracy: 0.3314 - f1: 0.6632\n",
      "Epoch 47/50\n",
      "709/709 [==============================] - 1320s 2s/step - loss: 2.2619 - accuracy: 0.3316 - f1: 0.6633\n",
      "Epoch 48/50\n",
      "709/709 [==============================] - 16922s 24s/step - loss: 2.2895 - accuracy: 0.3324 - f1: 0.6631\n",
      "Epoch 49/50\n",
      "709/709 [==============================] - 8002s 11s/step - loss: 2.2774 - accuracy: 0.3304 - f1: 0.6632\n",
      "Epoch 50/50\n",
      "709/709 [==============================] - 1064s 1s/step - loss: 2.2656 - accuracy: 0.3317 - f1: 0.6633\n",
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaminigotimukul/anaconda3/envs/alivio/lib/python3.10/site-packages/keras/src/preprocessing/image.py:1137: UserWarning: Found 11314 invalid image filename(s) in x_col=\"building_id\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_csv\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_out\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 134\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_data, train_csv, test_data, test_csv, model_out, model_in)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#Evalulate f1 weighted scores on validation set\u001b[39;00m\n\u001b[1;32m    133\u001b[0m validation_gen \u001b[38;5;241m=\u001b[39m validation_generator(test_csv, test_data)\n\u001b[0;32m--> 134\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_gen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m val_trues \u001b[38;5;241m=\u001b[39m validation_gen\u001b[38;5;241m.\u001b[39mclasses\n\u001b[1;32m    137\u001b[0m val_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/alivio/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/alivio/lib/python3.10/site-packages/keras/src/preprocessing/image.py:103\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to retrieve element \u001b[39m\u001b[38;5;132;01m{idx}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut the Sequence \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas length \u001b[39m\u001b[38;5;132;01m{length}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx\u001b[38;5;241m=\u001b[39midx, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    args.train_data, args.train_csv, args.test_data, args.test_csv, args.model_out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26d815ba-589b-4e1e-a853-ef7999bdd643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaminigotimukul/anaconda3/envs/alivio/lib/python3.10/site-packages/keras/src/preprocessing/image.py:1137: UserWarning: Found 11314 invalid image filename(s) in x_col=\"building_id\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m generate_xBD_baseline_model()\n\u001b[1;32m      2\u001b[0m validation_gen \u001b[38;5;241m=\u001b[39m validation_generator(test_csv_labels, TEST_DAT_DIR)\n\u001b[0;32m----> 3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_gen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m val_trues \u001b[38;5;241m=\u001b[39m validation_gen\u001b[38;5;241m.\u001b[39mclasses\n\u001b[1;32m      6\u001b[0m val_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/alivio/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/alivio/lib/python3.10/site-packages/keras/src/preprocessing/image.py:103\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to retrieve element \u001b[39m\u001b[38;5;132;01m{idx}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut the Sequence \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas length \u001b[39m\u001b[38;5;132;01m{length}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx\u001b[38;5;241m=\u001b[39midx, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "model = generate_xBD_baseline_model()\n",
    "validation_gen = validation_generator(test_csv_labels, TEST_DAT_DIR)\n",
    "predictions = model.predict(validation_gen)\n",
    "\n",
    "val_trues = validation_gen.classes\n",
    "val_pred = np.argmax(predictions, axis=-1)\n",
    "\n",
    "f1_weighted = f1_score(val_trues, val_pred, average='weighted')\n",
    "print(f1_weighted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowevn",
   "language": "python",
   "name": "tensorflowevn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
