{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f26954-c292-4e91-813e-a31797d4d2a4",
   "metadata": {},
   "source": [
    "### Loading Cleaned Metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e037c093-2122-42f3-b0c1-ff901878f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import ViTModel, ViTImageProcessor, ViTForImageClassification\n",
    "from timm import create_model\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05db6911-62c3-42c4-8a9d-f287709af45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/src/04_models\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Path : \", os.getcwd())\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdd561fc-8761-4782-9b25-10b08fd5c76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Project Path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio\n",
      "Root Data Path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage\n",
      "Train Data Path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/train\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "print(\"Root Project Path : \", ROOT_DIR)\n",
    "ROOT_DATA_DIR = os.path.join(ROOT_DIR, \"data\", \"xview_building_damage\")\n",
    "print(\"Root Data Path : \", ROOT_DATA_DIR)\n",
    "TRAIN_DATA_DIR = os.path.join(ROOT_DATA_DIR, \"train\")\n",
    "print(\"Train Data Path : \", TRAIN_DATA_DIR)\n",
    "CHALLENGE_DIR = os.path.join(ROOT_DATA_DIR, \"challenge\")\n",
    "TRAIN_DIR = os.path.join(CHALLENGE_DIR, \"train\")\n",
    "HOLD_DIR = os.path.join(CHALLENGE_DIR, \"hold\")\n",
    "TEST_DIR = os.path.join(CHALLENGE_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197816a-5517-4af3-b3d7-3300092a770d",
   "metadata": {},
   "source": [
    "### Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7ef52e-3918-4ac5-937c-0e59937d05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        ROOT_DATA_DIR,\n",
    "        \"challenge\",\n",
    "        \"train\",\n",
    "        \"disaster\",\n",
    "        \"hc_train_hurricane-michael.csv\",\n",
    "    )\n",
    ")\n",
    "valid_meta_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        ROOT_DATA_DIR, \"challenge\", \"hold\", \"disaster\", \"hc_hold_hurricane-michael.csv\"\n",
    "    )\n",
    ")\n",
    "test_meta_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        ROOT_DATA_DIR, \"challenge\", \"test\", \"disaster\", \"hc_test_hurricane-michael.csv\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c889dc-127b-4861-81b0-daaef7f1c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bec13b-009d-456d-ab04-0dae534cd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        if batch_idx >= 3:\n",
    "            break\n",
    "        print(\" Batch index:\", batch_idx, end=\"\")\n",
    "        print(\" | Batch size:\", y.shape[0], end=\"\")\n",
    "        print(\" | x shape:\", x.shape, end=\"\")\n",
    "        print(\" | y shape:\", y.shape)\n",
    "\n",
    "print(\"Labels from current batch:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa44fd3-0fb5-4bce-a9fa-ef564256b595",
   "metadata": {},
   "source": [
    "### Setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af0b1d5-c7ed-46e0-9d5f-43e87dc90639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  mps\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vit_base_patch16_224\"\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ddd8b-0f2b-4650-a427-9a48c2c4c5fe",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f5647-410b-4f1e-b418-b99a5fb870df",
   "metadata": {},
   "source": [
    "#### Data Augumentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b5accc0-c18f-45b3-8854-88e99570cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write transform for image\n",
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        # Resize the images to 64x64\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        # Flip the images randomly on the horizontal\n",
    "        # transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
    "        # Turn the image into a torch.Tensor\n",
    "        transforms.ToTensor(),  # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad1b3a-9e4b-4dae-91f2-e95f334a4b69",
   "metadata": {},
   "source": [
    "#### Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "764e2c5b-e2ab-4344-a91f-6d0c46ae3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_class_dir(disaster_name, dataSplit, ROOT_DIR):\n",
    "    DIR = os.path.join(ROOT_DIR, dataSplit, \"disaster\", disaster_name, \"class\", \"post\")\n",
    "    return DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52db5dc9-b46e-402d-9178-a04a8cb7fd0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set records :  22686\n",
      "Validation set records :  7158\n",
      "Test set records :  5657\n",
      "================================================\n",
      "Train data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 22686\n",
      "    Root location: /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/train/disaster/hurricane-michael/class/post\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n",
      "Validation data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 7158\n",
      "    Root location: /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/hold/disaster/hurricane-michael/class/post\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n",
      "Test data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 5657\n",
      "    Root location: /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/test/disaster/hurricane-michael/class/post\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "    get_img_class_dir(\"hurricane-michael\", \"train\", CHALLENGE_DIR),\n",
    "    transform=data_transform,\n",
    ")\n",
    "valid_dataset = datasets.ImageFolder(\n",
    "    get_img_class_dir(\"hurricane-michael\", \"hold\", CHALLENGE_DIR),\n",
    "    transform=data_transform,\n",
    ")\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    get_img_class_dir(\"hurricane-michael\", \"test\", CHALLENGE_DIR),\n",
    "    transform=data_transform,\n",
    ")\n",
    "# dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "print(\"Train set records : \", len(train_dataset))\n",
    "print(\"Validation set records : \", len(valid_dataset))\n",
    "print(\"Test set records : \", len(test_dataset))\n",
    "print(\"================================================\")\n",
    "print(f\"Train data:\\n{train_dataset}\")\n",
    "print(f\"Validation data:\\n{valid_dataset}\")\n",
    "print(f\"Test data:\\n{test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b93e9b68-e0ba-49b1-bf6a-52f72a1f2829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['destroyed', 'major-damage', 'minor-damage', 'no-damage', 'un-classified']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2061add4-decf-4b93-9c0c-e8a648c5779b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'destroyed': 0,\n",
       " 'major-damage': 1,\n",
       " 'minor-damage': 2,\n",
       " 'no-damage': 3,\n",
       " 'un-classified': 4}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = train_dataset.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1cc9ba4-beb0-4898-8d4c-2f279207ac1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Image shape: torch.Size([3, 224, 224])\n",
      "Image datatype: torch.float32\n",
      "Image label: 0\n",
      "Label datatype: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "img, label = train_dataset[0][0], train_dataset[0][1]\n",
    "print(f\"Image tensor:\\n{img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afec6e7f-e9d3-461a-ae65-8b3b97d1c0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([3, 224, 224]) -> [color_channels, height, width]\n",
      "Image permute shape: torch.Size([224, 224, 3]) -> [height, width, color_channels]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rearrange the order of dimensions\n",
    "img_permute = img.permute(1, 2, 0)\n",
    "\n",
    "# Print out different shapes (before and after permute)\n",
    "print(f\"Original shape: {img.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Image permute shape: {img_permute.shape} -> [height, width, color_channels]\")\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(class_names[label], fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40c69a-a9b9-4a83-b87e-a088f5b8733e",
   "metadata": {},
   "source": [
    "#### Spliting the train into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0321798-d1c6-4206-bc0f-7578386c92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.Subset(train_dataset, list(range(len(train_dataset))))\n",
    "testset = torch.utils.data.Subset(test_dataset, list(range(len(test_dataset))))\n",
    "validset = torch.utils.data.Subset(valid_dataset, list(range(len(valid_dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af29299c-3441-4134-bdd6-c5fff74d1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# get training indices that wil be used for validation\n",
    "# train_size = len(trainset)\n",
    "# indices = list(range(train_size))\n",
    "# np.random.shuffle(indices)\n",
    "# split = int(np.floor(valid_size * train_size))\n",
    "# train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers to obtain training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, num_workers=2)\n",
    "valid_loader = DataLoader(validset, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2642e-ed13-487e-9a4c-f4969e243ce5",
   "metadata": {},
   "source": [
    "#### Class Statistics - Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "999674f1-0830-4c6d-b048-ee89516f5525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples per classes in training dataset:\n",
      "\n",
      "\t 0: 757\n",
      "\t 1: 1902\n",
      "\t 2: 5207\n",
      "\t 3: 14588\n",
      "\t 4: 232\n"
     ]
    }
   ],
   "source": [
    "# print out classes statistics\n",
    "\n",
    "# get all training samples labels\n",
    "train_labels = [labels for i, (images, labels) in enumerate(train_loader)]\n",
    "train_labels = torch.cat((train_labels), 0)\n",
    "train_labels_count = train_labels.unique(return_counts=True)\n",
    "\n",
    "# # print(train_labels_count)\n",
    "\n",
    "print(\"The number of samples per classes in training dataset:\\n\")\n",
    "for label, count in zip(train_labels_count[0], train_labels_count[1]):\n",
    "    print(\"\\t {}: {}\".format(label, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62415d49-4c41-4f50-b910-fae2b4f79418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples per classes in valid dataset:\n",
      "\n",
      "\t 0: 285\n",
      "\t 1: 608\n",
      "\t 2: 1722\n",
      "\t 3: 4457\n",
      "\t 4: 86\n"
     ]
    }
   ],
   "source": [
    "# get all test samples labels\n",
    "valid_labels = [labels for i, (images, labels) in enumerate(valid_loader)]\n",
    "valid_labels = torch.cat((valid_labels), 0)\n",
    "valid_labels_count = valid_labels.unique(return_counts=True)\n",
    "\n",
    "print(\"The number of samples per classes in valid dataset:\\n\")\n",
    "for label, count in zip(valid_labels_count[0], valid_labels_count[1]):\n",
    "    print(\"\\t {}: {}\".format(label, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b2797c7-c0b5-446b-8d7d-2f85c36c861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of samples per classes in test dataset:\n",
      "\n",
      "\t 0: 183\n",
      "\t 1: 409\n",
      "\t 2: 1363\n",
      "\t 3: 3647\n",
      "\t 4: 55\n"
     ]
    }
   ],
   "source": [
    "# get all test samples labels\n",
    "test_labels = [labels for i, (images, labels) in enumerate(test_loader)]\n",
    "test_labels = torch.cat((test_labels), 0)\n",
    "test_labels_count = test_labels.unique(return_counts=True)\n",
    "\n",
    "print()\n",
    "print(\"The number of samples per classes in test dataset:\\n\")\n",
    "for label, count in zip(test_labels_count[0], test_labels_count[1]):\n",
    "    print(\"\\t {}: {}\".format(label, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe69d9c-f0e8-42c1-8158-e177a0668ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cec26-f550-4197-aa4a-5f1685d9ecc1",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4410c932-074e-46eb-ab21-f54cb879fdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_transformer = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "vision_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef007a56-8af9-439a-bd8a-8f7246c254ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vit-16 model is trained on ImageNet\n",
    "# we expect to have output of 1000 number of classes\n",
    "\n",
    "vision_transformer.heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bbab752-dd9e-4f34-a698-8d617a4ef00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune with dataset\n",
    "\n",
    "# change the number of output classes\n",
    "vision_transformer.heads = nn.Linear(\n",
    "    in_features=768, out_features=len(class_names), bias=True\n",
    ")\n",
    "\n",
    "# freeze the parameters except the last linear layer\n",
    "#\n",
    "# freeze weights\n",
    "for p in vision_transformer.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# unfreeze weights of classification head to train\n",
    "for p in vision_transformer.heads.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e7fd5ac-4bc6-4780-a237-3703685d79b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: class_token, Frozen: True\n",
      "\n",
      "Layer Name: conv_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: conv_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.pos_embedding, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_0.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_1.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_2.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_3.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_4.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_5.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_6.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_7.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_8.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_9.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_10.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.ln_1.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.ln_1.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.self_attention.in_proj_weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.self_attention.in_proj_bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.self_attention.out_proj.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.self_attention.out_proj.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.ln_2.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.ln_2.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.mlp.0.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.mlp.0.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.mlp.3.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.layers.encoder_layer_11.mlp.3.bias, Frozen: True\n",
      "\n",
      "Layer Name: encoder.ln.weight, Frozen: True\n",
      "\n",
      "Layer Name: encoder.ln.bias, Frozen: True\n",
      "\n",
      "Layer Name: heads.weight, Frozen: False\n",
      "\n",
      "Layer Name: heads.bias, Frozen: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check whether corresponding layers are frozen\n",
    "\n",
    "for layer_name, p in vision_transformer.named_parameters():\n",
    "    print(\"Layer Name: {}, Frozen: {}\".format(layer_name, not p.requires_grad))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6f99b19-5e54-4908-b304-9509e00af5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizer\n",
    "# only train the parameters with requires_grad set to True\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, vision_transformer.parameters()), lr=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70084c7f-15e7-442a-84e8-df70ee4b73a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_gpu = torch.backends.mps.is_available()\n",
    "train_on_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a6b8071-d6ea-4434-82c1-a4c8c6ca7f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available :  mps\n",
      "Epoch: 1 \t Training Loss: 0.413383 \t Validation Loss: 1.892839\n",
      "Epoch: 2 \t Training Loss: 0.384764 \t Validation Loss: 1.930643\n",
      "Epoch: 3 \t Training Loss: 0.383249 \t Validation Loss: 1.940685\n",
      "Epoch: 4 \t Training Loss: 0.380062 \t Validation Loss: 1.943661\n",
      "Epoch: 5 \t Training Loss: 0.378085 \t Validation Loss: 1.944971\n",
      "Epoch: 6 \t Training Loss: 0.376817 \t Validation Loss: 1.945760\n",
      "Epoch: 7 \t Training Loss: 0.375892 \t Validation Loss: 1.946336\n",
      "Epoch: 8 \t Training Loss: 0.375153 \t Validation Loss: 1.946810\n",
      "Epoch: 9 \t Training Loss: 0.374532 \t Validation Loss: 1.947240\n",
      "Epoch: 10 \t Training Loss: 0.373999 \t Validation Loss: 1.947661\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "# number of epochs\n",
    "n_epoch = 10\n",
    "\n",
    "train_loss_list, valid_loss_list = [], []\n",
    "\n",
    "# move model to GPU\n",
    "if train_on_gpu:\n",
    "    print(\"Device available : \", device)\n",
    "    vision_transformer.to(device)\n",
    "\n",
    "# prepare model for training\n",
    "vision_transformer.train()\n",
    "\n",
    "for e in range(n_epoch):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    # get batch data\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "\n",
    "        # move to gpu if available\n",
    "        if train_on_gpu:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # clear grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feedforward data\n",
    "        outputs = vision_transformer(images)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # backward pass, calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # track loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    vision_transformer.eval()\n",
    "\n",
    "    # validate model\n",
    "    for images, targets in valid_loader:\n",
    "\n",
    "        # move to gpu if available\n",
    "        if train_on_gpu:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "        # turn off gradients\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = vision_transformer(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    # set model back to training mode\n",
    "    vision_transformer.train()\n",
    "\n",
    "    # get average loss values\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    valid_loss = valid_loss / len(valid_loader)\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "\n",
    "    # output training statistics for epoch\n",
    "    print(\n",
    "        \"Epoch: {} \\t Training Loss: {:.6f} \\t Validation Loss: {:.6f}\".format(\n",
    "            (e + 1), train_loss, valid_loss\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed283e7c-cfc6-41e1-bd87-bce88f384080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHjElEQVR4nO3deVxVdf7H8fcF5AIiuLMogvsumguDjltZhg6T2ZSZKWpmlppmNmXmWsnYaqlplmmbuY1ak6ahk5pm45KYlZqOGymiZqwqKvf8/uDHHa+A7Fw4vp6Px3lwz/d8zzmfI8h9c873nGsxDMMQAACASbg4uwAAAIDiRLgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBStHgwYMVEhJSqHWnTp0qi8VSvAWVMcePH5fFYtHixYtLfd8Wi0VTp061zy9evFgWi0XHjx/Pc92QkBANHjy4WOspys8KcKsj3ADKfGPLz7R582Znl3rLe/LJJ2WxWHTkyJFc+0ycOFEWi0U//vhjKVZWcKdPn9bUqVMVGxvr7FLssgLma6+95uxSgEJzc3YBQFnw8ccfO8x/9NFHiomJydbetGnTIu3nvffek81mK9S6L7zwgp577rki7d8MBgwYoNmzZ2vJkiWaPHlyjn0+++wztWzZUq1atSr0fgYOHKgHH3xQVqu10NvIy+nTpzVt2jSFhISodevWDsuK8rMC3OoIN4Ckhx9+2GH++++/V0xMTLb2G128eFFeXl753k+FChUKVZ8kubm5yc2N/7JhYWFq0KCBPvvssxzDzY4dO3Ts2DH94x//KNJ+XF1d5erqWqRtFEVRflaAWx2XpYB86tatm1q0aKE9e/aoS5cu8vLy0vPPPy9J+vzzz9W7d28FBgbKarWqfv36evHFF5WRkeGwjRvHUVx/CWDBggWqX7++rFar2rdvr127djmsm9OYG4vFolGjRmnNmjVq0aKFrFarmjdvrvXr12erf/PmzWrXrp08PDxUv359vfvuu/kex/Ptt9/q/vvvV506dWS1WhUUFKSnnnpKly5dynZ83t7eOnXqlPr06SNvb2/VqFFD48ePz/ZvkZiYqMGDB8vX11eVK1dWVFSUEhMT86xFyjx7c/DgQf3www/Zli1ZskQWi0X9+/fXlStXNHnyZLVt21a+vr6qWLGiOnfurG+++SbPfeQ05sYwDL300kuqXbu2vLy81L17d/3888/Z1r1w4YLGjx+vli1bytvbWz4+PoqIiNC+ffvsfTZv3qz27dtLkoYMGWK/9Jk13iinMTdpaWl6+umnFRQUJKvVqsaNG+u1116TYRgO/Qryc1FYZ8+e1SOPPCI/Pz95eHgoNDRUH374YbZ+S5cuVdu2bVWpUiX5+PioZcuWeuutt+zLr169qmnTpqlhw4by8PBQtWrV9Oc//1kxMTHFVituPfwZCBTA77//roiICD344IN6+OGH5efnJynzjdDb21vjxo2Tt7e3/v3vf2vy5MlKTk7Wq6++mud2lyxZopSUFD322GOyWCx65ZVX1LdvXx09ejTPv+C3bdumVatW6YknnlClSpX09ttv67777tPJkydVrVo1SdLevXt19913KyAgQNOmTVNGRoamT5+uGjVq5Ou4V6xYoYsXL+rxxx9XtWrVtHPnTs2ePVu//fabVqxY4dA3IyNDPXv2VFhYmF577TVt3LhRr7/+uurXr6/HH39cUmZIuOeee7Rt2zaNGDFCTZs21erVqxUVFZWvegYMGKBp06ZpyZIluu222xz2vXz5cnXu3Fl16tTR+fPn9f7776t///569NFHlZKSooULF6pnz57auXNntktBeZk8ebJeeukl9erVS7169dIPP/ygu+66S1euXHHod/ToUa1Zs0b333+/6tatq4SEBL377rvq2rWrfvnlFwUGBqpp06aaPn26Jk+erOHDh6tz586SpI4dO+a4b8Mw9Ne//lXffPONHnnkEbVu3VobNmzQM888o1OnTunNN9906J+fn4vCunTpkrp166YjR45o1KhRqlu3rlasWKHBgwcrMTFRY8aMkSTFxMSof//+uuOOOzRz5kxJ0oEDB7R9+3Z7n6lTpyo6OlrDhg1Thw4dlJycrN27d+uHH37QnXfeWaQ6cQszAGQzcuRI48b/Hl27djUkGfPnz8/W/+LFi9naHnvsMcPLy8u4fPmyvS0qKsoIDg62zx87dsyQZFSrVs24cOGCvf3zzz83JBn/+te/7G1TpkzJVpMkw93d3Thy5Ii9bd++fYYkY/bs2fa2yMhIw8vLyzh16pS97fDhw4abm1u2beYkp+OLjo42LBaLceLECYfjk2RMnz7doW+bNm2Mtm3b2ufXrFljSDJeeeUVe9u1a9eMzp07G5KMRYsW5VlT+/btjdq1axsZGRn2tvXr1xuSjHfffde+zfT0dIf1/vjjD8PPz88YOnSoQ7skY8qUKfb5RYsWGZKMY8eOGYZhGGfPnjXc3d2N3r17Gzabzd7v+eefNyQZUVFR9rbLly871GUYmd9rq9Xq8G+za9euXI/3xp+VrH+zl156yaHf3/72N8NisTj8DOT35yInWT+Tr776aq59Zs2aZUgyPvnkE3vblStXjPDwcMPb29tITk42DMMwxowZY/j4+BjXrl3LdVuhoaFG7969b1oTUFBclgIKwGq1asiQIdnaPT097a9TUlJ0/vx5de7cWRcvXtTBgwfz3G6/fv1UpUoV+3zWX/FHjx7Nc90ePXqofv369vlWrVrJx8fHvm5GRoY2btyoPn36KDAw0N6vQYMGioiIyHP7kuPxpaWl6fz58+rYsaMMw9DevXuz9R8xYoTDfOfOnR2OZd26dXJzc7OfyZEyx7iMHj06X/VImeOkfvvtN23dutXetmTJErm7u+v++++3b9Pd3V2SZLPZdOHCBV27dk3t2rXL8ZLWzWzcuFFXrlzR6NGjHS7ljR07Nltfq9UqF5fMX68ZGRn6/fff5e3trcaNGxd4v1nWrVsnV1dXPfnkkw7tTz/9tAzD0FdffeXQntfPRVGsW7dO/v7+6t+/v72tQoUKevLJJ5WamqotW7ZIkipXrqy0tLSbXmKqXLmyfv75Zx0+fLjIdQFZCDdAAdSqVcv+Znm9n3/+Wffee698fX3l4+OjGjVq2AcjJyUl5bndOnXqOMxnBZ0//vijwOtmrZ+17tmzZ3Xp0iU1aNAgW7+c2nJy8uRJDR48WFWrVrWPo+natauk7Mfn4eGR7XLX9fVI0okTJxQQECBvb2+Hfo0bN85XPZL04IMPytXVVUuWLJEkXb58WatXr1ZERIRDUPzwww/VqlUr+3iOGjVqaO3atfn6vlzvxIkTkqSGDRs6tNeoUcNhf1JmkHrzzTfVsGFDWa1WVa9eXTVq1NCPP/5Y4P1ev//AwEBVqlTJoT3rDr6s+rLk9XNRFCdOnFDDhg3tAS63Wp544gk1atRIERERql27toYOHZpt3M/06dOVmJioRo0aqWXLlnrmmWfK/C38KPsIN0ABXH8GI0tiYqK6du2qffv2afr06frXv/6lmJgY+xiD/NzOm9tdOcYNA0WLe938yMjI0J133qm1a9fq2Wef1Zo1axQTE2Mf+Hrj8ZXWHUY1a9bUnXfeqX/+85+6evWq/vWvfyklJUUDBgyw9/nkk080ePBg1a9fXwsXLtT69esVExOj22+/vURvs54xY4bGjRunLl266JNPPtGGDRsUExOj5s2bl9rt3SX9c5EfNWvWVGxsrL744gv7eKGIiAiHsVVdunTRf//7X33wwQdq0aKF3n//fd122216//33S61OmA8DioEi2rx5s37//XetWrVKXbp0sbcfO3bMiVX9T82aNeXh4ZHjQ+9u9iC8LPv379evv/6qDz/8UIMGDbK3F+VuluDgYG3atEmpqakOZ28OHTpUoO0MGDBA69ev11dffaUlS5bIx8dHkZGR9uUrV65UvXr1tGrVKodLSVOmTClUzZJ0+PBh1atXz95+7ty5bGdDVq5cqe7du2vhwoUO7YmJiapevbp9viBPnA4ODtbGjRuVkpLicPYm67JnVn2lITg4WD/++KNsNpvD2ZucanF3d1dkZKQiIyNls9n0xBNP6N1339WkSZPsZw6rVq2qIUOGaMiQIUpNTVWXLl00depUDRs2rNSOCebCmRugiLL+Qr7+L+IrV67onXfecVZJDlxdXdWjRw+tWbNGp0+ftrcfOXIk2ziN3NaXHI/PMAyH23kLqlevXrp27ZrmzZtnb8vIyNDs2bMLtJ0+ffrIy8tL77zzjr766iv17dtXHh4eN639P//5j3bs2FHgmnv06KEKFSpo9uzZDtubNWtWtr6urq7ZzpCsWLFCp06dcmirWLGiJOXrFvhevXopIyNDc+bMcWh/8803ZbFY8j1+qjj06tVLZ86c0bJly+xt165d0+zZs+Xt7W2/ZPn77787rOfi4mJ/sGJ6enqOfby9vdWgQQP7cqAwOHMDFFHHjh1VpUoVRUVF2T8a4OOPPy7V0/95mTp1qr7++mt16tRJjz/+uP1NskWLFnk++r9JkyaqX7++xo8fr1OnTsnHx0f//Oc/izR2IzIyUp06ddJzzz2n48ePq1mzZlq1alWBx6N4e3urT58+9nE311+SkqS//OUvWrVqle6991717t1bx44d0/z589WsWTOlpqYWaF9Zz+uJjo7WX/7yF/Xq1Ut79+7VV1995XA2Jmu/06dP15AhQ9SxY0ft379fn376qcMZH0mqX7++KleurPnz56tSpUqqWLGiwsLCVLdu3Wz7j4yMVPfu3TVx4kQdP35coaGh+vrrr/X5559r7NixDoOHi8OmTZt0+fLlbO19+vTR8OHD9e6772rw4MHas2ePQkJCtHLlSm3fvl2zZs2yn1kaNmyYLly4oNtvv121a9fWiRMnNHv2bLVu3do+PqdZs2bq1q2b2rZtq6pVq2r37t1auXKlRo0aVazHg1uMc27SAsq23G4Fb968eY79t2/fbvzpT38yPD09jcDAQOPvf/+7sWHDBkOS8c0339j75XYreE633eqGW5NzuxV85MiR2dYNDg52uDXZMAxj06ZNRps2bQx3d3ejfv36xvvvv288/fTThoeHRy7/Cv/zyy+/GD169DC8vb2N6tWrG48++qj91uLrb2OOiooyKlasmG39nGr//fffjYEDBxo+Pj6Gr6+vMXDgQGPv3r35vhU8y9q1aw1JRkBAQLbbr202mzFjxgwjODjYsFqtRps2bYwvv/wy2/fBMPK+FdwwDCMjI8OYNm2aERAQYHh6ehrdunUzfvrpp2z/3pcvXzaefvppe79OnToZO3bsMLp27Wp07drVYb+ff/650axZM/tt+VnHnlONKSkpxlNPPWUEBgYaFSpUMBo2bGi8+uqrDremZx1Lfn8ubpT1M5nb9PHHHxuGYRgJCQnGkCFDjOrVqxvu7u5Gy5Yts33fVq5cadx1111GzZo1DXd3d6NOnTrGY489ZsTHx9v7vPTSS0aHDh2MypUrG56enkaTJk2Ml19+2bhy5cpN6wRuxmIYZejPSwClqk+fPtyGC8B0GHMD3CJu/KiEw4cPa926derWrZtzCgKAEsKZG+AWERAQoMGDB6tevXo6ceKE5s2bp/T0dO3duzfbs1sAoDxjQDFwi7j77rv12Wef6cyZM7JarQoPD9eMGTMINgBMhzM3AADAVBhzAwAATIVwAwAATOWWG3Njs9l0+vRpVapUqUCPPgcAAM5jGIZSUlIUGBiY7UNbb3TLhZvTp08rKCjI2WUAAIBCiIuLU+3atW/ax6nhJjo6WqtWrdLBgwfl6empjh07aubMmWrcuPFN11uxYoUmTZqk48ePq2HDhpo5c6Z69eqVr31mPRY8Li5OPj4+RT4GAABQ8pKTkxUUFOTwwbG5cWq42bJli0aOHKn27dvr2rVrev7553XXXXfpl19+sX+g3I2+++479e/f3/75LkuWLFGfPn30ww8/qEWLFnnuM+tSlI+PD+EGAIByJj9DSsrUreDnzp1TzZo1tWXLFnXp0iXHPv369VNaWpq+/PJLe9uf/vQntW7dWvPnz89zH8nJyfL19VVSUhLhBgCAcqIg799l6m6prE8Erlq1aq59duzYoR49eji09ezZUzt27CjR2gAAQPlQZgYU22w2jR07Vp06dbrp5aUzZ87Iz8/Poc3Pz09nzpzJsX96errS09Pt88nJycVTMAAAKJPKTLgZOXKkfvrpJ23btq1YtxsdHa1p06YV6zYBAGWDzWbTlStXnF0Giom7u3uet3nnR5kIN6NGjdKXX36prVu35nl7l7+/vxISEhzaEhIS5O/vn2P/CRMmaNy4cfb5rNHWAIDy7cqVKzp27JhsNpuzS0ExcXFxUd26deXu7l6k7Tg13BiGodGjR2v16tXavHmz6tatm+c64eHh2rRpk8aOHWtvi4mJUXh4eI79rVarrFZrcZUMACgDDMNQfHy8XF1dFRQUVCx/7cO5sh6yGx8frzp16hTpQbtODTcjR47UkiVL9Pnnn6tSpUr2cTO+vr7y9PSUJA0aNEi1atVSdHS0JGnMmDHq2rWrXn/9dfXu3VtLly7V7t27tWDBAqcdBwCgdF27dk0XL15UYGCgvLy8nF0OikmNGjV0+vRpXbt2TRUqVCj0dpwadefNm6ekpCR169ZNAQEB9mnZsmX2PidPnlR8fLx9vmPHjlqyZIkWLFig0NBQrVy5UmvWrMnXM24AAOaQkZEhSUW+fIGyJev7mfX9LSynX5bKy+bNm7O13X///br//vtLoCIAQHnCZwSaS3F9P7lICQAATIVwAwBAORYSEqJZs2Y5u4wyhXADAEApsFgsN52mTp1aqO3u2rVLw4cPL95iy7ky8ZwbAADKvTzGkcafPmV/vWzZck2eMkWHDh6wt3l7e0uGTZIhwzCUkZEhN7f/f5u+yaZrVKuS+cJ2LT9F5qPPDX3ztcqNnSySq/MGexNugOJm2CQjw3GyXbuh7f/nbTfM59o/w/5LTzL+9zrXrzdZltf6Rh59cux/49c8asvajuQ4n+21CrbMuG67Oa5TwGU3q9Vh/vr1dJO269e7SVth1smtpvzWmW17OcznuY98bLMw7TnVVSFIqv2qlHhJcrfcsE5+5NHXyPaiyPyve6/3dftDFtnk735akrR52x517zNC65bO0gsz5mv/gSP6esUcBdXy07hJb+r7PT8p7eIlNW0YouhJI9Wja5h9WyFt/qqxjz2osSMekiRZqrfXe29O1NqY7drwzQ7V8q+p16eP0V8juhbbseTJpYJUJbT09ncDwg3KPluGlHFRupbmOGVkvb6YQ1vW/GVlCw+2HMKEce26oJGfsHGT5cX4yxBALiy+kj1gZ+adixcLMtIiv3fl5N3Py8um4rpp67kX5+q1aWNUL7iWqlSupLhTCerVo5Nenvi4rO7u+mj5WkUOeFqHvl+pOrVzfjK/JE179T29MuVJvTr1Sc1+f5kGjJisE7FfqGoV3yJUZ3H4ctN+FueOeiHcoHhkXMkeLLKFkZsFlJz6XxdQTMMiubhJFtfrphvmc1zu8v+/LCzZv2a9tlgk3fj1Jv1zXK8E+jusl7X8utdSwZdZrlt+4zqFWVbYehzmr9+mcu+T1/Js28jndnPdV362ccO+8t2eS98ibTOf+7pikxKuSj7BkodVF9Mk7xoecobU5KuqWDGnJZacGjN57ZMsrlKV1pnzlZIkSdNfnKk77/mrvVvVelJol372+Rfb36PV6/+jL7Ye06hRvTIbXdwlryCp6m32foOHPKr+jz4vSZrR9E69vaCSdh6+orvvbntDiTepsRwj3CCTYUiXE6TkA1LSAenymZsHjhsnIz/XeovKIrl5SW4VJdeKmV9zmq5f5uohe0BwKWigyG15PtbNdV/m/EUClLrLl6VzxyRXa+b/c1cn1uJSoeC357j8f8Eubg7z7Tp0+N8ySampqZo6darWrl2r+Ph4Xbt2TZcuXdLJuDg5nB2xWBzmW4WG2n/fVPT2lo+Pj86eO3fL/A4i3NxqDJuUdvL/Q8wv/wszyQekK38UffsWt7wDR67LvG7e39XzlvmPCaBgvLyk1FTn7bu4VLzhFND48eMVExOj1157TQ0aNJCnp6f+9re/5flJ6Dd+dIHFYrmlPmCUcGNWtqtSyhHH8JL0i5R8KPPyUI4sknc9yaepVLGO5OadSxjxyj2kOHF0PIBbl8WiXC4NlW/bt2/X4MGDde+990rKPJNz/Phx5xZVDhBuyrtrFzMDy41nYlIO536pyKWCVKmR5NssM8j4NJV8m2a2uXmWbv0AgFw1bNhQq1atUmRkpCwWiyZNmnRLnYEpLMJNeXHlj+vOwFx3JibthHK9O8et4nXhpVlmgPFpmnl2xoVvPQCUdW+88YaGDh2qjh07qnr16nr22WeVnJzs7LLKPIuRn0+vNJHk5GT5+voqKSlJPj4+zi7HkWFkDuR1uIx03QDf3FirST7XhZesMONVmzEqAEzp8uXLOnbsmOrWrSsPD+fcJYXid7Pva0Hev/nz3RkMW+YZlxsH9Cb9Il1Nyn09r9o5n4nxqFF6tQMAUMYRbkpSxhUp9Uj2MzHJh6SMSzmvY3GRKtZzDC8+TSXfJlKFMnamCQCAMohwU1yuJEqnvnQ8E5Ny5CaDeq2ST6P/nYWxD+ptmPnMBgAAUCiEm+Jy5Q9px8Ds7W6V/ncGxve6S0oV6zo8qAkAABQPwk1xqRgs+feQvOs7nonxrMWgXgAAShHhprhYXKTbY5xdBQAAtzznfmwnAABAMSPcAAAAUyHcAAAAUyHcAABQTnTr1k1jx461z4eEhGjWrFk3XcdisWjNmjVF3ndxbac0EG4AACgFkZGRuvvuu3Nc9u2338pisejHH38s0DZ37dql4cOHF0d5dlOnTlXr1q2ztcfHxysiIqJY91VSCDcAAJSCRx55RDExMfrtt9+yLVu0aJHatWunVq1aFWibNWrUkJeXV3GVeFP+/v6yWq2lsq+iItwAAFAK/vKXv6hGjRpavHixQ3tqaqpWrFihPn36qH///qpVq5a8vLzUsmVLffbZZzfd5o2XpQ4fPqwuXbrIw8NDzZo1U0xM9keUPPvss2rUqJG8vLxUr149TZo0SVevXpUkLV68WNOmTdO+fftksVhksVjs9d54WWr//v26/fbb5enpqWrVqmn48OFKTU21Lx88eLD69Omj1157TQEBAapWrZpGjhxp31dJ4jk3AIDyzzCkjIvO2berV74e1urm5qZBgwZp8eLFmjhxoiz/v86KFSuUkZGhhx9+WCtWrNCzzz4rHx8frV27VgMHDlT9+vXVoUOHPLdvs9nUt29f+fn56T//+Y+SkpIcxudkqVSpkhYvXqzAwEDt379fjz76qCpVqqS///3v6tevn3766SetX79eGzdulCT5+vpm20ZaWpp69uyp8PBw7dq1S2fPntWwYcM0atQoh/D2zTffKCAgQN98842OHDmifv36qXXr1nr00UfzPJ6iINwAAMq/jIvScm/n7PuBVMmtYr66Dh06VK+++qq2bNmibt26Scq8JHXfffcpODhY48ePt/cdPXq0NmzYoOXLl+cr3GzcuFEHDx7Uhg0bFBgYKEmaMWNGtnEyL7zwgv11SEiIxo8fr6VLl+rvf/+7PD095e3tLTc3N/n7++e6ryVLlujy5cv66KOPVLFi5rHPmTNHkZGRmjlzpvz8/CRJVapU0Zw5c+Tq6qomTZqod+/e2rRpU4mHGy5LAQBQSpo0aaKOHTvqgw8+kCQdOXJE3377rR555BFlZGToxRdfVMuWLVW1alV5e3trw4YNOnnyZL62feDAAQUFBdmDjSSFh4dn67ds2TJ16tRJ/v7+8vb21gsvvJDvfVy/r9DQUHuwkaROnTrJZrPp0KFD9rbmzZvL1fV/n6MYEBCgs2fPFmhfhcGZGwBA+efqlXkGxVn7LoBHHnlEo0eP1ty5c7Vo0SLVr19fXbt21cyZM/XWW29p1qxZatmypSpWrKixY8fqypUrxVbqjh07NGDAAE2bNk09e/aUr6+vli5dqtdff73Y9nG9ChUqOMxbLBbZbLYS2df1CDcAgPLPYsn3pSFne+CBBzRmzBgtWbJEH330kR5//HFZLBZt375d99xzjx5++GFJmWNofv31VzVr1ixf223atKni4uIUHx+vgIAASdL333/v0Oe7775TcHCwJk6caG87ceKEQx93d3dlZGTkua/FixcrLS3NfvZm+/btcnFxUePGjfNVb0nishQAAKXI29tb/fr104QJExQfH6/BgwdLkho2bKiYmBh99913OnDggB577DElJCTke7s9evRQo0aNFBUVpX379unbb791CDFZ+zh58qSWLl2q//73v3r77be1evVqhz4hISE6duyYYmNjdf78eaWnp2fb14ABA+Th4aGoqCj99NNP+uabbzR69GgNHDjQPt7GmZwabrZu3arIyEgFBgbm+8mHn376qUJDQ+Xl5aWAgAANHTpUv//+e8kXCwBAMXnkkUf0xx9/qGfPnvYxMi+88IJuu+029ezZU926dZO/v7/69OmT7226uLho9erVunTpkjp06KBhw4bp5Zdfdujz17/+VU899ZRGjRql1q1b67vvvtOkSZMc+tx33326++671b17d9WoUSPH29G9vLy0YcMGXbhwQe3bt9ff/vY33XHHHZozZ07B/zFKgMUwDMNZO//qq6+0fft2tW3bVn379tXq1atv+o3cvn27unTpojfffFORkZE6deqURowYoUaNGmnVqlX52mdycrJ8fX2VlJQkHx+fYjoSAEBpunz5so4dO6a6devKw8PD2eWgmNzs+1qQ92+njrmJiIgo0KOcd+zYoZCQED355JOSpLp16+qxxx7TzJkzS6pEAABQzpSrMTfh4eGKi4vTunXrZBiGEhIStHLlSvXq1SvXddLT05WcnOwwAQAA8ypX4aZTp0769NNP1a9fP7m7u8vf31++vr6aO3durutER0fL19fXPgUFBZVixQAAoLSVq3Dzyy+/aMyYMZo8ebL27Nmj9evX6/jx4xoxYkSu60yYMEFJSUn2KS4urhQrBgAApa1cPecmOjpanTp10jPPPCNJatWqlSpWrKjOnTvrpZdest/Xfz2r1VpuPsUUAFAwTrwnBiWguL6f5erMzcWLF+Xi4lhy1mOd+QEHgFtH1u/+4nx6L5wv6/t5/Uc2FIZTz9ykpqbqyJEj9vmshwZVrVpVderU0YQJE3Tq1Cl99NFHkqTIyEg9+uijmjdvnnr27Kn4+HiNHTtWHTp0cPgsDQCAubm5ucnLy0vnzp1ThQoVsv3hi/LHZrPp3Llz8vLykptb0eKJU8PN7t271b17d/v8uHHjJElRUVFavHix4uPjHT7Ma/DgwUpJSdGcOXP09NNPq3Llyrr99tu5FRwAbjEWi0UBAQE6duxYto8PQPnl4uKiOnXqyGKxFGk7Tn2InzPwED8AMA+bzcalKRNxd3fP9SxcuXmIHwAAReHi4sITipENFykBAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpEG4AAICpODXcbN26VZGRkQoMDJTFYtGaNWvyXCc9PV0TJ05UcHCwrFarQkJC9MEHH5R8sQAAoFxwc+bO09LSFBoaqqFDh6pv3775WueBBx5QQkKCFi5cqAYNGig+Pl42m62EKwUAAOWFU8NNRESEIiIi8t1//fr12rJli44ePaqqVatKkkJCQkqoOgAAUB6VqzE3X3zxhdq1a6dXXnlFtWrVUqNGjTR+/HhdunQp13XS09OVnJzsMAEAAPNy6pmbgjp69Ki2bdsmDw8PrV69WufPn9cTTzyh33//XYsWLcpxnejoaE2bNq2UKwUAAM5Srs7c2Gw2WSwWffrpp+rQoYN69eqlN954Qx9++GGuZ28mTJigpKQk+xQXF1fKVQMAgNJUrs7cBAQEqFatWvL19bW3NW3aVIZh6LffflPDhg2zrWO1WmW1WkuzTAAA4ETl6sxNp06ddPr0aaWmptrbfv31V7m4uKh27dpOrAwAAJQVTg03qampio2NVWxsrCTp2LFjio2N1cmTJyVlXlIaNGiQvf9DDz2katWqaciQIfrll1+0detWPfPMMxo6dKg8PT2dcQgAAKCMcWq42b17t9q0aaM2bdpIksaNG6c2bdpo8uTJkqT4+Hh70JEkb29vxcTEKDExUe3atdOAAQMUGRmpt99+2yn1AwCAssdiGIbh7CJKU3Jysnx9fZWUlCQfHx9nlwMAAPKhIO/f5WrMDQAAQF4INwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFScGm62bt2qyMhIBQYGymKxaM2aNfled/v27XJzc1Pr1q1LrD4AAFD+ODXcpKWlKTQ0VHPnzi3QeomJiRo0aJDuuOOOEqoMAACUV27O3HlERIQiIiIKvN6IESP00EMPydXVtUBnewAAgPmVuzE3ixYt0tGjRzVlypR89U9PT1dycrLDBAAAzKtchZvDhw/rueee0yeffCI3t/yddIqOjpavr699CgoKKuEqAQCAM5WbcJORkaGHHnpI06ZNU6NGjfK93oQJE5SUlGSf4uLiSrBKAADgbE4dc1MQKSkp2r17t/bu3atRo0ZJkmw2mwzDkJubm77++mvdfvvt2dazWq2yWq2lXS4AAHCSchNufHx8tH//foe2d955R//+97+1cuVK1a1b10mVAQCAssSp4SY1NVVHjhyxzx87dkyxsbGqWrWq6tSpowkTJujUqVP66KOP5OLiohYtWjisX7NmTXl4eGRrBwAAty6nhpvdu3ere/fu9vlx48ZJkqKiorR48WLFx8fr5MmTzioPAACUQxbDMAxnF1GakpOT5evrq6SkJPn4+Di7HAAAkA8Fef8uN3dLAQAA5AfhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmIpTw83WrVsVGRmpwMBAWSwWrVmz5qb9V61apTvvvFM1atSQj4+PwsPDtWHDhtIpFgAAlAuFCjdxcXH67bff7PM7d+7U2LFjtWDBggJtJy0tTaGhoZo7d26++m/dulV33nmn1q1bpz179qh79+6KjIzU3r17C7RfAABgXhbDMIyCrtS5c2cNHz5cAwcO1JkzZ9S4cWM1b95chw8f1ujRozV58uSCF2KxaPXq1erTp0+B1mvevLn69euX730mJyfL19dXSUlJ8vHxKXCdAACg9BXk/btQZ25++ukndejQQZK0fPlytWjRQt99950+/fRTLV68uDCbLBSbzaaUlBRVrVq11PYJAADKNrfCrHT16lVZrVZJ0saNG/XXv/5VktSkSRPFx8cXX3V5eO2115SamqoHHngg1z7p6elKT0+3zycnJ5dGaQAAwEkKdeamefPmmj9/vr799lvFxMTo7rvvliSdPn1a1apVK9YCc7NkyRJNmzZNy5cvV82aNXPtFx0dLV9fX/sUFBRUKvUBAADnKFS4mTlzpt59911169ZN/fv3V2hoqCTpiy++sF+uKklLly7VsGHDtHz5cvXo0eOmfSdMmKCkpCT7FBcXV+L1AQAA5ynUZalu3brp/PnzSk5OVpUqVeztw4cPl5eXV7EVl5PPPvtMQ4cO1dKlS9W7d+88+1utVvslNAAAYH6FCjeXLl2SYRj2YHPixAmtXr1aTZs2Vc+ePfO9ndTUVB05csQ+f+zYMcXGxqpq1aqqU6eOJkyYoFOnTumjjz6SlHkpKioqSm+99ZbCwsJ05swZSZKnp6d8fX0LcygAAMBkCnVZ6p577rEHjsTERIWFhen1119Xnz59NG/evHxvZ/fu3WrTpo3atGkjSRo3bpzatGljv607Pj5eJ0+etPdfsGCBrl27ppEjRyogIMA+jRkzpjCHAQAATKhQz7mpXr26tmzZoubNm+v999/X7NmztXfvXv3zn//U5MmTdeDAgZKotVjwnBsAAMqfEn/OzcWLF1WpUiVJ0tdff62+ffvKxcVFf/rTn3TixInCbBIAAKBYFCrcNGjQQGvWrFFcXJw2bNigu+66S5J09uxZzoYAAACnKlS4mTx5ssaPH6+QkBB16NBB4eHhkjLP4mSNnwEAAHCGQo25kaQzZ84oPj5eoaGhcnHJzEg7d+6Uj4+PmjRpUqxFFifG3AAAUP4U5P27ULeCS5K/v7/8/f3tnw5eu3btUnmAHwAAwM0U6rKUzWbT9OnT5evrq+DgYAUHB6ty5cp68cUXZbPZirtGAACAfCvUmZuJEydq4cKF+sc//qFOnTpJkrZt26apU6fq8uXLevnll4u1SAAAgPwq1JibwMBAzZ8/3/5p4Fk+//xzPfHEEzp16lSxFVjcGHMDAED5U+LPublw4UKOg4abNGmiCxcuFGaTAAAAxaJQ4SY0NFRz5szJ1j5nzhy1atWqyEUBAAAUVqHG3Lzyyivq3bu3Nm7caH/GzY4dOxQXF6d169YVa4EAAAAFUagzN127dtWvv/6qe++9V4mJiUpMTFTfvn31888/6+OPPy7uGgEAAPKt0A/xy8m+fft02223KSMjo7g2WewYUAwAQPlT4gOKAQAAyirCDQAAMBXCDQAAMJUC3S3Vt2/fmy5PTEwsSi0AAABFVqBw4+vrm+fyQYMGFakgAACAoihQuFm0aFFJ1QEAAFAsGHMDAABMhXADAABMhXADAABMhXADAABMhXADAABMhXADAABMhXADAABMhXADAABMhXADAABMhXADAABMhXADAABMhXADAABMhXADAABMxanhZuvWrYqMjFRgYKAsFovWrFmT5zqbN2/WbbfdJqvVqgYNGmjx4sUlXicAACg/nBpu0tLSFBoaqrlz5+ar/7Fjx9S7d291795dsbGxGjt2rIYNG6YNGzaUcKUAAKC8cHPmziMiIhQREZHv/vPnz1fdunX1+uuvS5KaNm2qbdu26c0331TPnj1LqkwAAFCOlKsxNzt27FCPHj0c2nr27KkdO3bkuk56erqSk5MdJgAAYF7lKtycOXNGfn5+Dm1+fn5KTk7WpUuXclwnOjpavr6+9ikoKKg0SgUAAE5SrsJNYUyYMEFJSUn2KS4uztklAQCAEuTUMTcF5e/vr4SEBIe2hIQE+fj4yNPTM8d1rFarrFZraZQHAADKgHJ15iY8PFybNm1yaIuJiVF4eLiTKgIAAGWNU8NNamqqYmNjFRsbKynzVu/Y2FidPHlSUuYlpUGDBtn7jxgxQkePHtXf//53HTx4UO+8846WL1+up556yhnlAwCAMsip4Wb37t1q06aN2rRpI0kaN26c2rRpo8mTJ0uS4uPj7UFHkurWrau1a9cqJiZGoaGhev311/X+++9zGzgAALCzGIZhOLuI0pScnCxfX18lJSXJx8fH2eUAAIB8KMj7d7kacwMAAJAXwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADCVMhFu5s6dq5CQEHl4eCgsLEw7d+68af9Zs2apcePG8vT0VFBQkJ566ildvny5lKoFAABlmdPDzbJlyzRu3DhNmTJFP/zwg0JDQ9WzZ0+dPXs2x/5LlizRc889pylTpujAgQNauHChli1bpueff76UKwcAAGWR08PNG2+8oUcffVRDhgxRs2bNNH/+fHl5eemDDz7Isf93332nTp066aGHHlJISIjuuusu9e/fP8+zPQAA4Nbg1HBz5coV7dmzRz169LC3ubi4qEePHtqxY0eO63Ts2FF79uyxh5mjR49q3bp16tWrV47909PTlZyc7DABAADzcnPmzs+fP6+MjAz5+fk5tPv5+engwYM5rvPQQw/p/Pnz+vOf/yzDMHTt2jWNGDEi18tS0dHRmjZtWrHXDgAAyianX5YqqM2bN2vGjBl655139MMPP2jVqlVau3atXnzxxRz7T5gwQUlJSfYpLi6ulCsGAAClyalnbqpXry5XV1clJCQ4tCckJMjf3z/HdSZNmqSBAwdq2LBhkqSWLVsqLS1Nw4cP18SJE+Xi4pjXrFarrFZryRwAAAAoc5x65sbd3V1t27bVpk2b7G02m02bNm1SeHh4jutcvHgxW4BxdXWVJBmGUXLFAgCAcsGpZ24kady4cYqKilK7du3UoUMHzZo1S2lpaRoyZIgkadCgQapVq5aio6MlSZGRkXrjjTfUpk0bhYWF6ciRI5o0aZIiIyPtIQcAANy6nB5u+vXrp3Pnzmny5Mk6c+aMWrdurfXr19sHGZ88edLhTM0LL7wgi8WiF154QadOnVKNGjUUGRmpl19+2VmHAAAAyhCLcYtdy0lOTpavr6+SkpLk4+Pj7HIAAEA+FOT9u9zdLQUAAHAzhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqhBsAAGAqZSLczJ07VyEhIfLw8FBYWJh27tx50/6JiYkaOXKkAgICZLVa1ahRI61bt66UqgUAAGWZm7MLWLZsmcaNG6f58+crLCxMs2bNUs+ePXXo0CHVrFkzW/8rV67ozjvvVM2aNbVy5UrVqlVLJ06cUOXKlUu/eAAAUOZYDMMwnFlAWFiY2rdvrzlz5kiSbDabgoKCNHr0aD333HPZ+s+fP1+vvvqqDh48qAoVKhR4f8nJyfL19VVSUpJ8fHyKXD8AACh5BXn/duplqStXrmjPnj3q0aOHvc3FxUU9evTQjh07clzniy++UHh4uEaOHCk/Pz+1aNFCM2bMUEZGRmmVDQAAyjCnXpY6f/68MjIy5Ofn59Du5+engwcP5rjO0aNH9e9//1sDBgzQunXrdOTIET3xxBO6evWqpkyZkq1/enq60tPT7fPJycnFexAAAKBMKRMDigvCZrOpZs2aWrBggdq2bat+/fpp4sSJmj9/fo79o6Oj5evra5+CgoJKuWIAAFCanBpuqlevLldXVyUkJDi0JyQkyN/fP8d1AgIC1KhRI7m6utrbmjZtqjNnzujKlSvZ+k+YMEFJSUn2KS4urngPAgAAlClODTfu7u5q27atNm3aZG+z2WzatGmTwsPDc1ynU6dOOnLkiGw2m73t119/VUBAgNzd3bP1t1qt8vHxcZgAAIB5Of2y1Lhx4/Tee+/pww8/1IEDB/T4448rLS1NQ4YMkSQNGjRIEyZMsPd//PHHdeHCBY0ZM0a//vqr1q5dqxkzZmjkyJHOOgQAAFCGOP05N/369dO5c+c0efJknTlzRq1bt9b69evtg4xPnjwpF5f/ZbCgoCBt2LBBTz31lFq1aqVatWppzJgxevbZZ511CAAAoAxx+nNuShvPuQEAoPwpN8+5AQAAKG6EGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCplItzMnTtXISEh8vDwUFhYmHbu3Jmv9ZYuXSqLxaI+ffqUbIH5YLNJjz8uRUdLS5ZI27dLp05ltgMAgNLj5uwCli1bpnHjxmn+/PkKCwvTrFmz1LNnTx06dEg1a9bMdb3jx49r/Pjx6ty5cylWm7szZ6T587O3V6ggBQVJwcGZU0jI/14HB2cuq1Ch1MsFAMC0LIZhGM4sICwsTO3bt9ecOXMkSTabTUFBQRo9erSee+65HNfJyMhQly5dNHToUH377bdKTEzUmjVr8rW/5ORk+fr6KikpST4+PsV1GDp3Tpo9WzpxQjp+PPPrb79JGRk3X8/FRQoMdAw8NwYgT89iKxMAgHKpIO/fTj1zc+XKFe3Zs0cTJkywt7m4uKhHjx7asWNHrutNnz5dNWvW1COPPKJvv/32pvtIT09Xenq6fT45ObnoheegRg1p+nTHtmvXpNOnHQPPjVN6emYI+u23zEtZOalZ0zHs3BiAfH1L5JAAACiXnBpuzp8/r4yMDPn5+Tm0+/n56eDBgzmus23bNi1cuFCxsbH52kd0dLSmTZtW1FILxc1NqlMnc8rp6pnNJp09+7+gk1MASknJ7HP2rLRrV8778fXNfrbn+vnq1SWLpQQPFACAMsTpY24KIiUlRQMHDtR7772n6tWr52udCRMmaNy4cfb55ORkBQUFlVSJBeLiIvn7Z05hYdmXG4b0xx/ZA8/1Iej336WkJGnfvswpJ15emQErtwAUEJBZCwAAZuDUcFO9enW5uroqISHBoT0hIUH+/v7Z+v/3v//V8ePHFRkZaW+z/f/tSG5ubjp06JDq16/vsI7VapXVai2B6kuexSJVrZo5tWmTc5/U1Jwvd2UFoPh46eJF6eDBzCkn1w96zgo81atnBh6LJfNrUV4XxzaK8trNTXJ1zf41qw8AwFycGm7c3d3Vtm1bbdq0yX47t81m06ZNmzRq1Khs/Zs0aaL9+/c7tL3wwgtKSUnRW2+9VWbOyJQmb2+pefPMKSfp6VJcXO6XveLipKtXpaNHM6dbTU6h58av+elTGtvICm0uLtnn87OsoO3FvQ5hEkBpcfplqXHjxikqKkrt2rVThw4dNGvWLKWlpWnIkCGSpEGDBqlWrVqKjo6Wh4eHWrRo4bB+5cqVJSlbOzJZrVKDBplTTq4f9Hx9AEpMzLwsZrP972tJvy6JbWfN5yYjI+872lC8bjyrl9vrsr68pKfS2k/WJJX/tuvnc3ud337F9boo6xe0raT7F2Qbbm5S7dpyGqeHm379+uncuXOaPHmyzpw5o9atW2v9+vX2QcYnT56UCwNCSkxeg57NwGb7X4i5dq1oX4tjG4XdVlZoy8hwDG9ZU27tpbFOQb8fEqESMLOAgMw/nJ3F6c+5KW0l9Zwb4FZmGPkPSjeeqTOMstuWn+VmmbK+j3m1ldW+18/n9jq//YrrdVHWv/FraS8r6vr+/sU/1KHcPOcGgDlYLJlnAQGgLOB6DwAAMBXCDQAAMBXCDQAAMBXCDQAAMBXCDQAAMBXCDQAAMBXCDQAAMBXCDQAAMBXCDQAAMBXCDQAAMBXCDQAAMBXCDQAAMBXCDQAAMBXCDQAAMBU3ZxdQ2gzDkCQlJyc7uRIAAJBfWe/bWe/jN3PLhZuUlBRJUlBQkJMrAQAABZWSkiJfX9+b9rEY+YlAJmKz2XT69GlVqlRJFoulWLednJysoKAgxcXFycfHp1i3XZo4jrKF4yhbzHIcknmOheMoW0rqOAzDUEpKigIDA+XicvNRNbfcmRsXFxfVrl27RPfh4+NTrn8ws3AcZQvHUbaY5Tgk8xwLx1G2lMRx5HXGJgsDigEAgKkQbgAAgKkQboqR1WrVlClTZLVanV1KkXAcZQvHUbaY5Tgk8xwLx1G2lIXjuOUGFAMAAHPjzA0AADAVwg0AADAVwg0AADAVwg0AADAVwk0x2Lp1qyIjIxUYGCiLxaI1a9Y4u6RCiY6OVvv27VWpUiXVrFlTffr00aFDh5xdVoHNmzdPrVq1sj9AKjw8XF999ZWzyyqyf/zjH7JYLBo7dqyzSymQqVOnymKxOExNmjRxdlmFcurUKT388MOqVq2aPD091bJlS+3evdvZZRVISEhItu+HxWLRyJEjnV1agWRkZGjSpEmqW7euPD09Vb9+fb344ov5+tyhsiYlJUVjx45VcHCwPD091bFjR+3atcvZZeUpr/c+wzA0efJkBQQEyNPTUz169NDhw4dLpTbCTTFIS0tTaGio5s6d6+xSimTLli0aOXKkvv/+e8XExOjq1au66667lJaW5uzSCqR27dr6xz/+oT179mj37t26/fbbdc899+jnn392dmmFtmvXLr377rtq1aqVs0splObNmys+Pt4+bdu2zdklFdgff/yhTp06qUKFCvrqq6/0yy+/6PXXX1eVKlWcXVqB7Nq1y+F7ERMTI0m6//77nVxZwcycOVPz5s3TnDlzdODAAc2cOVOvvPKKZs+e7ezSCmzYsGGKiYnRxx9/rP379+uuu+5Sjx49dOrUKWeXdlN5vfe98sorevvttzV//nz95z//UcWKFdWzZ09dvny55IszUKwkGatXr3Z2GcXi7NmzhiRjy5Ytzi6lyKpUqWK8//77zi6jUFJSUoyGDRsaMTExRteuXY0xY8Y4u6QCmTJlihEaGursMors2WefNf785z87u4xiN2bMGKN+/fqGzWZzdikF0rt3b2Po0KEObX379jUGDBjgpIoK5+LFi4arq6vx5ZdfOrTfdtttxsSJE51UVcHd+N5ns9kMf39/49VXX7W3JSYmGlar1fjss89KvB7O3CBXSUlJkqSqVas6uZLCy8jI0NKlS5WWlqbw8HBnl1MoI0eOVO/evdWjRw9nl1Johw8fVmBgoOrVq6cBAwbo5MmTzi6pwL744gu1a9dO999/v2rWrKk2bdrovffec3ZZRXLlyhV98sknGjp0aLF/kHBJ69ixozZt2qRff/1VkrRv3z5t27ZNERERTq6sYK5du6aMjAx5eHg4tHt6epbLM5xZjh07pjNnzjj83vL19VVYWJh27NhR4vu/5T44E/ljs9k0duxYderUSS1atHB2OQW2f/9+hYeH6/Lly/L29tbq1avVrFkzZ5dVYEuXLtUPP/xQLq6/5yYsLEyLFy9W48aNFR8fr2nTpqlz58766aefVKlSJWeXl29Hjx7VvHnzNG7cOD3//PPatWuXnnzySbm7uysqKsrZ5RXKmjVrlJiYqMGDBzu7lAJ77rnnlJycrCZNmsjV1VUZGRl6+eWXNWDAAGeXViCVKlVSeHi4XnzxRTVt2lR+fn767LPPtGPHDjVo0MDZ5RXamTNnJEl+fn4O7X5+fvZlJYlwgxyNHDlSP/30U7n9y6Fx48aKjY1VUlKSVq5cqaioKG3ZsqVcBZy4uDiNGTNGMTEx2f6qK0+u/0u6VatWCgsLU3BwsJYvX65HHnnEiZUVjM1mU7t27TRjxgxJUps2bfTTTz9p/vz55TbcLFy4UBEREQoMDHR2KQW2fPlyffrpp1qyZImaN2+u2NhYjR07VoGBgeXu+/Hxxx9r6NChqlWrllxdXXXbbbepf//+2rNnj7NLK7e4LIVsRo0apS+//FLffPONateu7exyCsXd3V0NGjRQ27ZtFR0drdDQUL311lvOLqtA9uzZo7Nnz+q2226Tm5ub3NzctGXLFr399ttyc3NTRkaGs0sslMqVK6tRo0Y6cuSIs0spkICAgGzhuGnTpuXyEpsknThxQhs3btSwYcOcXUqhPPPMM3ruuef04IMPqmXLlho4cKCeeuopRUdHO7u0Aqtfv762bNmi1NRUxcXFaefOnbp69arq1avn7NIKzd/fX5KUkJDg0J6QkGBfVpIIN7AzDEOjRo3S6tWr9e9//1t169Z1dknFxmazKT093dllFMgdd9yh/fv3KzY21j61a9dOAwYMUGxsrFxdXZ1dYqGkpqbqv//9rwICApxdSoF06tQp26MRfv31VwUHBzupoqJZtGiRatasqd69ezu7lEK5ePGiXFwc38JcXV1ls9mcVFHRVaxYUQEBAfrjjz+0YcMG3XPPPc4uqdDq1q0rf39/bdq0yd6WnJys//znP6Uy/pHLUsUgNTXV4a/QY8eOKTY2VlWrVlWdOnWcWFnBjBw5UkuWLNHnn3+uSpUq2a+L+vr6ytPT08nV5d+ECRMUERGhOnXqKCUlRUuWLNHmzZu1YcMGZ5dWIJUqVco23qlixYqqVq1auRoHNX78eEVGRio4OFinT5/WlClT5Orqqv79+zu7tAJ56qmn1LFjR82YMUMPPPCAdu7cqQULFmjBggXOLq3AbDabFi1apKioKLm5lc+3gcjISL388suqU6eOmjdvrr179+qNN97Q0KFDnV1agW3YsEGGYahx48Y6cuSInnnmGTVp0kRDhgxxdmk3ldd739ixY/XSSy+pYcOGqlu3riZNmqTAwED16dOn5Isr8fuxbgHffPONISnbFBUV5ezSCiSnY5BkLFq0yNmlFcjQoUON4OBgw93d3ahRo4Zxxx13GF9//bWzyyoW5fFW8H79+hkBAQGGu7u7UatWLaNfv37GkSNHnF1WofzrX/8yWrRoYVitVqNJkybGggULnF1SoWzYsMGQZBw6dMjZpRRacnKyMWbMGKNOnTqGh4eHUa9ePWPixIlGenq6s0srsGXLlhn16tUz3N3dDX9/f2PkyJFGYmKis8vKU17vfTabzZg0aZLh5+dnWK1W44477ii1nzmLYZTDxzkCAADkgjE3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3ACDJYrFozZo1zi4DQDEg3ABwusGDB8tisWSb7r77bmeXBqAcKp8fKgLAdO6++24tWrTIoc1qtTqpGgDlGWduAJQJVqtV/v7+DlOVKlUkZV4ymjdvniIiIuTp6al69epp5cqVDuvv379ft99+uzw9PVWtWjUNHz5cqampDn0++OADNW/eXFarVQEBARo1apTD8vPnz+vee++Vl5eXGjZsqC+++KJkDxpAiSDcACgXJk2apPvuu0/79u3TgAED9OCDD+rAgQOSpLS0NPXs2VNVqlTRrl27tGLFCm3cuNEhvMybN08jR47U8OHDtX//fn3xxRdq0KCBwz6mTZumBx54QD/++KN69eqlAQMG6MKFC6V6nACKQal8PCcA3ERUVJTh6upqVKxY0WF6+eWXDcPI/MT6ESNGOKwTFhZmPP7444ZhGMaCBQuMKlWqGKmpqfbla9euNVxcXIwzZ84YhmEYgYGBxsSJE3OtQZLxwgsv2OdTU1MNScZXX31VbMcJoHQw5gZAmdC9e3fNmzfPoa1q1ar21+Hh4Q7LwsPDFRsbK0k6cOCAQkNDVbFiRfvyTp06yWaz6dChQ7JYLDp9+rTuuOOOm9bQqlUr++uKFSvKx8dHZ8+eLewhAXASwg2AMqFixYrZLhMVF09Pz3z1q1ChgsO8xWKRzWYriZIAlCDG3AAoF77//vts802bNpUkNW3aVPv27VNaWpp9+fbt2+Xi4qLGjRurUqVKCgkJ0aZNm0q1ZgDOwZkbAGVCenq6zpw549Dm5uam6tWrS5JWrFihdu3a6c9//rM+/fRT7dy5UwsXLpQkDRgwQFOmTFFUVJSmTp2qc+fOafTo0Ro4cKD8/PwkSVOnTtWIESNUs2ZNRUREKCUlRdu3b9fo0aNL90ABlDjCDYAyYf369QoICHBoa9y4sQ4ePCgp806mpUuX6oknnlBAQIA+++wzNWvWTJLk5eWlDRs2aMyYMWrfvr28vLx033336Y033rBvKyoqSpcvX9abb76p8ePHq3r16vrb3/5WegcIoNRYDMMwnF0EANyMxWLR6tWr1adPH2eXAqAcYMwNAAAwFcINAAAwFcbcACjzuHoOoCA4cwMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEzl/wBUmBrNYNcBRgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# plot losses\n",
    "x = list(range(1, n_epoch + 1))\n",
    "plt.plot(x, train_loss_list, color=\"blue\", label=\"Train\")\n",
    "plt.plot(x, valid_loss_list, color=\"orange\", label=\"Validation\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e532987-a68f-44f1-a38d-14d12b1cbe90",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f84eea9-8ef2-414f-8808-2cc607ccc4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.838110\n"
     ]
    }
   ],
   "source": [
    "# prepare model for evaluation\n",
    "vision_transformer.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "accuracy = 0\n",
    "\n",
    "# number of classes\n",
    "n_class = len(class_names)\n",
    "\n",
    "class_correct = np.zeros(n_class)\n",
    "class_total = np.zeros(n_class)\n",
    "\n",
    "# move model back to cpu\n",
    "vision_transformer = vision_transformer.to(\"cpu\")\n",
    "\n",
    "# test model\n",
    "for images, targets in test_loader:\n",
    "\n",
    "    # get outputs\n",
    "    outputs = vision_transformer(images)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # track loss\n",
    "    test_loss += loss.item()\n",
    "\n",
    "    # get predictions from probabilities\n",
    "    preds = torch.argmax(F.softmax(outputs, dim=1), dim=1)\n",
    "\n",
    "    # get correct predictions\n",
    "    correct_preds = (preds == targets).type(torch.FloatTensor)\n",
    "\n",
    "    # calculate and accumulate accuracy\n",
    "    accuracy += torch.mean(correct_preds).item() * 100\n",
    "\n",
    "    # calculate test accuracy for each class\n",
    "    for c in range(n_class):\n",
    "\n",
    "        targets = targets.to(\"cpu\")\n",
    "\n",
    "        class_total[c] += (targets == c).sum()\n",
    "        class_correct[c] += ((correct_preds) * (targets == c)).sum()\n",
    "\n",
    "# get average accuracy\n",
    "accuracy = accuracy / len(test_loader)\n",
    "\n",
    "# get average loss\n",
    "test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# output test loss statistics\n",
    "print(\"Test Loss: {:.6f}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76cd5c8f-30bc-4ce6-aab8-4a6c920aac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of Classes\n",
      "\n",
      "destroyed\t: 0% \t (0/183)\n",
      "major-damage\t: 0% \t (0/409)\n",
      "minor-damage\t: 0% \t (0/1363)\n",
      "no-damage\t: 100% \t (3647/3647)\n",
      "un-classified\t: 0% \t (0/55)\n",
      "\n",
      "Test Accuracy of Dataset: \t 64% \t (3647/5657)\n"
     ]
    }
   ],
   "source": [
    "class_accuracy = class_correct / class_total\n",
    "\n",
    "print(\"Test Accuracy of Classes\")\n",
    "print()\n",
    "\n",
    "for c in range(n_class):\n",
    "    print(\n",
    "        \"{}\\t: {}% \\t ({}/{})\".format(\n",
    "            class_names[c],\n",
    "            int(class_accuracy[c] * 100),\n",
    "            int(class_correct[c]),\n",
    "            int(class_total[c]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "print()\n",
    "print(\n",
    "    \"Test Accuracy of Dataset: \\t {}% \\t ({}/{})\".format(\n",
    "        int(accuracy), int(np.sum(class_correct)), int(np.sum(class_total))\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
