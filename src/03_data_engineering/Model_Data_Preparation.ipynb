{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d10995e6-bfb0-4d4a-8657-dab964a6631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os import path, walk, makedirs\n",
    "from skimage.io import imread\n",
    "from shapely import wkt\n",
    "from shapely.geometry import mapping, Polygon\n",
    "from cv2 import fillPoly, imwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda1f7d0-96c5-4059-b0a9-137e5903c3d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dir /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/src\n",
      "Current Dir /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Dir\",os.getcwd())\n",
    "os.chdir('../..')\n",
    "print(\"Current Dir\",os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22cbdf92-24ed-4ced-ba84-d0952f1feb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data\n",
      "DATA_DIR :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/train/sample\n",
      "SAM_CKPT_DIR :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/utils\n",
      "Metadata File path :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/train/xview2_processed.csv\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR=os.getcwd()\n",
    "BASE_DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "print(\"BASE_DIR : \", BASE_DATA_DIR)\n",
    "DATA_DIR = os.path.join(BASE_DATA_DIR, 'xview_building_damage/train/sample')\n",
    "print(\"DATA_DIR : \", DATA_DIR)\n",
    "SAM_CKPT_DIR=os.path.join(BASE_DATA_DIR, 'utils')\n",
    "print(\"SAM_CKPT_DIR : \", SAM_CKPT_DIR)\n",
    "metadata_csv_path=os.path.join(BASE_DATA_DIR, 'xview_building_damage/train/xview2_processed.csv')\n",
    "print(\"Metadata File path : \",metadata_csv_path )\n",
    "xbd_train = os.path.join(BASE_DATA_DIR, 'xview_building_damage', 'train', 'clean_meta_csv')\n",
    "\n",
    "bd_dir=os.path.join(BASE_DATA_DIR, 'xview_building_damage')\n",
    "challeng_dir = os.path.join(bd_dir, 'challenge')\n",
    "train_out_dir=os.path.join(challeng_dir, 'train', 'disaster')\n",
    "test_out_dir=os.path.join(challeng_dir, 'test', 'disaster')\n",
    "hold_out_dir=os.path.join(challeng_dir, 'hold', 'disaster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261cede-d87b-4fe4-82cf-b293543fa1f6",
   "metadata": {},
   "source": [
    "### Metafiles Cleanup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2edfba16-be3b-4ab4-bc4e-c3e80011a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_class_numeric_labels(df_name):\n",
    "    df_name['damage'].fillna('pre', inplace=True)\n",
    "    df_name['damage_class']=df_name['damage']\n",
    "    keys=list(df_name['damage_class'].value_counts().keys())\n",
    "    df_name['damage_class']=df_name['damage_class'].apply(keys.index)\n",
    "    df_name['damage_class'].value_counts()\n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "207f3e8f-4ea7-40c0-9463-d55f7ad988c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_meta_data(meta_df, out_dir, disaster_name ='hurricane-florence', dataset='train'):\n",
    "    hc_disaster = meta_df[meta_df['disaster']==disaster_name]\n",
    "    hc_train = hc_disaster[hc_disaster['dataset']== dataset]\n",
    "    df_disaster = hc_train[hc_train['image_polygon'].notna()]\n",
    "    df_disaster['mask_file_names'] = df_disaster['img_name'].str.replace('.png', '_')+df_disaster['building_id']+'.png'\n",
    "    df_disaster_class_labels = get_df_with_class_numeric_labels(df_disaster)\n",
    "    df_disaster_class_labels.to_csv(out_dir+'/hc_'+dataset+\"_\"+disaster_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43e14cc3-6915-44a3-858c-441bb30937a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(metadata_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e062e0-1138-4a8f-abf1-89c60ce809bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Michael\n",
    "pre_process_meta_data(df_all, train_out_dir, 'hurricane-michael', 'train')\n",
    "pre_process_meta_data(df_all, test_out_dir, 'hurricane-michael', 'test')\n",
    "pre_process_meta_data(df_all, hold_out_dir, 'hurricane-michael', 'hold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11bc08-bbf6-4f30-bd20-5138b0dd7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Florence\n",
    "pre_process_meta_data(df_all, xbd_train, 'hurricane-florence', 'train')\n",
    "pre_process_meta_data(df_all, xbd_train, 'hurricane-florence', 'test')\n",
    "pre_process_meta_data(df_all, xbd_train, 'hurricane-florence', 'hold')\n",
    "\n",
    "### Matthew\n",
    "pre_process_meta_data(df_all, xbd_train, 'hurricane-matthew', 'train')\n",
    "pre_process_meta_data(df_all, xbd_train, 'hurricane-matthew', 'test')\n",
    "pre_process_meta_data(df_all, xbd_train, 'hurricane-matthew', 'hold')\n",
    "\n",
    "### Harvey\n",
    "pre_process_meta_data(df_all, xbd_train, 'hurricane-harvey', 'train')\n",
    "pre_process_meta_data(df_all, xbd_train, 'hurricane-harvey', 'test')\n",
    "pre_process_meta_data(df_all, xbd_train, 'hurricane-harvey', 'hold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc193d18-4e0d-40ec-a757-9bb4c7e2dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hmc_hold=pd.read_csv(os.path.join(hold_out_dir, 'hc_hold_hurricane-michael.csv'))\n",
    "df_hmc_train=pd.read_csv(os.path.join(train_out_dir, 'hc_train_hurricane-michael.csv'))\n",
    "df_hmc_test=pd.read_csv(os.path.join(test_out_dir, 'hc_test_hurricane-michael.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee7956-50e9-4276-b844-5aaecb0234ff",
   "metadata": {},
   "source": [
    "### Spliting the images by disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9094199-8537-45ea-9721-e8b014c589aa",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaba1770-2380-4761-b8bc-c7371ed06def",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/utils/mask_generators/split_into_disasters.py --input data/xview_building_damage/challenge/train/raw --output data/xview_building_damage/challenge/train/disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15d5a1-59e8-4b88-baa4-01b258f32303",
   "metadata": {},
   "source": [
    "#### Hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "174a7644-31ff-4cc8-829f-1ecfa676c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/utils/mask_generators/split_into_disasters.py --input data/xview_building_damage/challenge/hold/raw --output data/xview_building_damage/challenge/hold/disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e9c91-10f2-4ec5-8253-00c1059448f0",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa01b88-283d-446e-8091-88a705f442fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/utils/mask_generators/split_into_disasters.py --input data/xview_building_damage/challenge/test/raw --output data/xview_building_damage/challenge/test/disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31978f-5b98-4622-859e-6727f0db97e6",
   "metadata": {},
   "source": [
    "### Creating Masks using Challenge Baseline Script\n",
    "\n",
    "This code is adapted from the original challenge baseline code base for generating masks and been modified slightly to generate the masks. The link for the original code is https://github.com/DIUx-xView/xView2_baseline/blob/master/utils/mask_polygons.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96f9fa58-1546-4a5e-9cbd-3436fbbf6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python src/utils/mask_generators/mask_polygons.py --input data/xview_building_damage/challenge/output --single-file --border 2\n",
    "### Adapting the contents of the mask_polygons.py below\n",
    "def get_dimensions(file_path):\n",
    "    \"\"\"\n",
    "    :param file_path: The path of the file \n",
    "    :return: returns (width,height,channels)\n",
    "    \"\"\"\n",
    "    # Open the image we are going to mask\n",
    "    pil_img = imread(file_path)\n",
    "    img = np.array(pil_img)\n",
    "    w, h, c = img.shape\n",
    "    return (w, h, c)\n",
    "\n",
    "def mask_polygons_separately(size, shapes):\n",
    "    \"\"\"\n",
    "    :param size: A tuple of the (width,height,channels)\n",
    "    :param shapes: A list of points in the polygon from get_feature_info\n",
    "    :returns: a dict of masked polygons with the shapes filled in from cv2.fillPoly\n",
    "    \"\"\"\n",
    "    # For each WKT polygon, read the WKT format and fill the polygon as an image\n",
    "    masked_polys = {}\n",
    "\n",
    "    for u in shapes:\n",
    "        sh = shapes[u]\n",
    "        mask_img = np.zeros(size, np.uint8)\n",
    "        i = fillPoly(mask_img, [sh], (255, 255, 255))\n",
    "        masked_polys[u] = i\n",
    "\n",
    "    return masked_polys\n",
    "\n",
    "def mask_polygons_together(size, shapes):\n",
    "    \"\"\"\n",
    "    :param size: A tuple of the (width,height,channels)\n",
    "    :param shapes: A list of points in the polygon from get_feature_info\n",
    "    :returns: A numpy array with the polygons filled 255s where there's a building and 0 where not \n",
    "    \"\"\"\n",
    "    # For each WKT polygon, read the WKT format and fill the polygon as an image\n",
    "    mask_img = np.zeros(size, np.uint8)\n",
    "\n",
    "    for u in shapes:\n",
    "        blank =  np.zeros(size, np.uint8)\n",
    "        poly = shapes[u] \n",
    "        fillPoly(blank, [poly], (1, 1, 1))\n",
    "        mask_img += blank\n",
    "    \n",
    "    # Here we are taking the overlap (+=) and squashing it back to 0\n",
    "    mask_img[mask_img > 1] = 0\n",
    "\n",
    "    # Finally we are taking all 1s and making it pure white (255)\n",
    "    mask_img[mask_img == 1] = 255\n",
    "\n",
    "    return mask_img\n",
    "\n",
    "def mask_polygons_together_with_border(size, shapes, border):\n",
    "    \"\"\"\n",
    "    :param size: A tuple of the (width,height,channels)\n",
    "    :param shapes: A list of points in the polygon from get_feature_info\n",
    "    :returns: a dict of masked polygons with the shapes filled in from cv2.fillPoly\n",
    "    \"\"\"\n",
    "\n",
    "    # For each WKT polygon, read the WKT format and fill the polygon as an image\n",
    "    mask_img = np.zeros(size, np.uint8)\n",
    "\n",
    "    for u in shapes:\n",
    "        blank =  np.zeros(size, np.uint8)\n",
    "        # Each polygon stored in shapes is a np.ndarray\n",
    "        poly = shapes[u]\n",
    "        \n",
    "        # Creating a shapely polygon object out of the numpy array \n",
    "        polygon = Polygon(poly)\n",
    "\n",
    "        # Getting the center points from the polygon and the polygon points\n",
    "        (poly_center_x, poly_center_y) = polygon.centroid.coords[0]\n",
    "        polygon_points = polygon.exterior.coords\n",
    "\n",
    "        # Setting a new polygon with each X,Y manipulated based off the center point\n",
    "        shrunk_polygon = []\n",
    "        for (x,y) in polygon_points:\n",
    "            if x < poly_center_x:\n",
    "                x += border\n",
    "            elif x > poly_center_x:\n",
    "                x -= border\n",
    "\n",
    "            if y < poly_center_y:\n",
    "                y += border\n",
    "            elif y > poly_center_y:\n",
    "                y -= border\n",
    "\n",
    "            shrunk_polygon.append([x,y])\n",
    "        \n",
    "        # Transforming the polygon back to a np.ndarray\n",
    "        ns_poly = np.array(shrunk_polygon, np.int32)\n",
    "  \n",
    "        # Filling the shrunken polygon to add a border between close polygons\n",
    "        fillPoly(blank, [ns_poly], (1, 1, 1))\n",
    "        mask_img += blank\n",
    "    \n",
    "    mask_img[mask_img > 1] = 0\n",
    "    mask_img[mask_img == 1] = 255\n",
    "    return mask_img\n",
    "\n",
    "def save_masks(masks, output_path, mask_file_name):\n",
    "    \"\"\"\n",
    "    :param masks: dictionary of UID:masked polygons from mask_polygons_separately()\n",
    "    :param output_path: path to save the masks\n",
    "    :param mask_file_name: the file name the masks should have \n",
    "    \"\"\"\n",
    "    # For each filled polygon, write out a separate file, increasing the name\n",
    "    for m in masks:\n",
    "        final_out = path.join(output_path,\n",
    "                              mask_file_name + '_{}.png'.format(m))\n",
    "        imwrite(final_out, masks[m])\n",
    "\n",
    "def save_one_mask(masks, output_path, mask_file_name):\n",
    "    \"\"\"\n",
    "    :param masks: list of masked polygons from the mask_polygons_separately function \n",
    "    :param output_path: path to save the masks\n",
    "    :param mask_file_name: the file name the masks should have \n",
    "    \"\"\"\n",
    "    # For each filled polygon, write the mask shape out to the file per image\n",
    "    mask_file_name = path.join(output_path, mask_file_name + '.png')\n",
    "    imwrite(mask_file_name, masks)\n",
    "    \n",
    "\n",
    "def read_json(json_path):\n",
    "    \"\"\"\n",
    "    :param json_path: path to load json from\n",
    "    :returns: a python dictionary of json features\n",
    "    \"\"\"\n",
    "    annotations = json.load(open(json_path))\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def get_feature_info(feature):\n",
    "    \"\"\"\n",
    "    :param feature: a python dictionary of json labels\n",
    "    :returns: a list mapping of polygons contained in the image \n",
    "    \"\"\"\n",
    "    # Getting each polygon points from the json file and adding it to a dictionary of uid:polygons\n",
    "    props = {}\n",
    "\n",
    "    for feat in feature['features']['xy']:\n",
    "        feat_shape = wkt.loads(feat['wkt'])\n",
    "        coords = list(mapping(feat_shape)['coordinates'][0])\n",
    "        props[feat['properties']['uid']] = (np.array(coords, np.int32))\n",
    "\n",
    "    return props\n",
    "\n",
    "\n",
    "def mask_chips(json_path, images_directory, output_directory, single_file, border, pre_post='_post'):\n",
    "    \"\"\"\n",
    "    :param json_path: path to find multiple json files for the chips\n",
    "    :param images_directory: path to the directory containing the images to be masked\n",
    "    :param output_directory: path to the directory where masks are to be saved\n",
    "    :param single_file: a boolean value to see if masks should be saved a single file or multiple\n",
    "    \"\"\"\n",
    "    # For each feature in the json we will create a separate mask\n",
    "    # Getting all files in the directory provided for jsons\n",
    "    jsons = [j for j in next(walk(json_path))[2] if pre_post in j]\n",
    "\n",
    "    # After removing non-json items in dir (if any)\n",
    "    for j in tqdm([j for j in jsons if j.endswith('json')],\n",
    "                  unit='poly',\n",
    "                  leave=False):\n",
    "        # Our chips start off in life as PNGs\n",
    "        chip_image_id = path.splitext(j)[0] + '.png'\n",
    "        mask_file = path.splitext(j)[0]\n",
    "\n",
    "        # Loading the per chip json\n",
    "        j_full_path = path.join(json_path, j)\n",
    "        chip_json = read_json(j_full_path)\n",
    "\n",
    "        # Getting the full chip path, and loading the size dimensions\n",
    "        chip_file = path.join(images_directory, chip_image_id)\n",
    "        chip_size = get_dimensions(chip_file)\n",
    "\n",
    "        # Reading in the polygons from the json file\n",
    "        polys = get_feature_info(chip_json)\n",
    "\n",
    "        # Getting a list of the polygons and saving masks as separate or single image files\n",
    "        if len(polys) > 0:\n",
    "            if single_file:\n",
    "                if border > 0:\n",
    "                    masked_polys = mask_polygons_together_with_border(chip_size, polys, border)\n",
    "                else:\n",
    "                    masked_polys = mask_polygons_together(chip_size, polys)\n",
    "                save_one_mask(masked_polys, output_directory, mask_file)\n",
    "            else:\n",
    "                masked_polys = mask_polygons_separately(chip_size, polys)\n",
    "                save_masks(masked_polys, output_directory, mask_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "71035e8a-b01d-42b4-b89f-5bc2c3c1cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(dataSplit='train', pre_post_str=\"post\", single_file=True):\n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\n",
    "        \"\"\"mask_polygons.py: Takes in xBD dataset and masks polygons in the image\\n\\n\n",
    "        WARNING: This could lead to hundreds of output images per input\\n\"\"\")\n",
    "    \n",
    "    parser.add_argument('--input',\n",
    "                        required=True,\n",
    "                        metavar=\"/path/to/xBD/\",\n",
    "                        help='Path to parent dataset directory \"xBD\"')\n",
    "    parser.add_argument('--single-file', \n",
    "                        action='store_true',\n",
    "                        help='use to save all masked polygon instances to a single file rather than one polygon per mask file')\n",
    "    parser.add_argument('--border',\n",
    "                        default=0,\n",
    "                        type=int,\n",
    "                        metavar=\"positive integer for pixel border (e.g. 1)\",\n",
    "                        help='Positive integer used to shrink the polygon by')\n",
    "    \n",
    "    if single_file: ### Creates all the masks in single file\n",
    "        args = parser.parse_args(['--input', 'data/xview_building_damage/challenge/output' , '--single-file', '--border', '2'])\n",
    "    else:\n",
    "        args = parser.parse_args(['--input', 'data/xview_building_damage/challenge/'+dataSplit+'/disaster' , '--border', '2'])\n",
    "    \n",
    "    # Getting the list of the disaster types under the xBD directory\n",
    "    disasters = next(walk(args.input))[1]\n",
    "    \n",
    "    for disaster in tqdm(disasters, desc='Masking', unit='disaster'):\n",
    "        # Create the full path to the images, labels, and mask output directories\n",
    "            print(\"disaster \", disaster)\n",
    "            image_dir = path.join(args.input, disaster, 'images')\n",
    "            json_dir = path.join(args.input, disaster, 'labels')\n",
    "            output_dir = path.join(args.input, disaster, 'masks', pre_post_str)\n",
    "        \n",
    "            if not path.isdir(image_dir):\n",
    "                print(\n",
    "                    \"Error, could not find image files in {}.\\n\\n\"\n",
    "                    .format(image_dir),\n",
    "                    file=stderr)\n",
    "                exit(2)\n",
    "        \n",
    "            if not path.isdir(json_dir):\n",
    "                print(\n",
    "                    \"Error, could not find labels in {}.\\n\\n\"\n",
    "                    .format(json_dir),\n",
    "                    file=stderr)\n",
    "                exit(3)\n",
    "        \n",
    "            if not path.isdir(output_dir):\n",
    "                makedirs(output_dir)\n",
    "        \n",
    "            mask_chips(json_dir, image_dir, output_dir, args.single_file, args.border, '_' + pre_post_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588c184-de7f-40a3-8478-3bfb8be89355",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_masks('train', \"post\", single_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ba93e-5b2e-417d-b1fe-2c6404f19f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_masks('train', \"post\", single_file=False)  ## Completed Locally\n",
    "create_masks('train', \"pre\", single_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe52cb2-3ec8-493e-9ca4-ac6ed6c8cf29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_masks('hold', \"pre\", single_file=False)\n",
    "create_masks('hold', \"post\", single_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813bffad-89e9-4d2b-8ea9-d5fb0c16401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_masks('test', \"pre\", single_file=False)\n",
    "create_masks('test', \"post\", single_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ccdf7-b476-40b7-95a9-7a6c10dfe26e",
   "metadata": {},
   "source": [
    "### Prepare for Pytorch Masks for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9c0dc7d6-a34e-4296-8fa0-7aa7fa8feac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_masks_by_class(top_dir, meta_df, disaster_name='hurricane-michael', post=True):\n",
    "    root_path=os.path.join(top_dir, disaster_name)\n",
    "    disas_post_mask= os.path.join(root_path,'masks', 'post') #source\n",
    "    print(\"Source root : \", disas_post_mask)\n",
    "    disas_class_path=os.path.join(root_path, 'class', 'post')\n",
    "    print(\"Destination root : \", disas_class_path)\n",
    "    if post :\n",
    "        df = meta_df[meta_df['is_post_image'] == post]\n",
    "    else:\n",
    "        df = meta_df[meta_df['is_pre_image'] != post]\n",
    "    print(\"Started moving the mask files to class folder for \", disaster_name)\n",
    "    for idx, file_name in enumerate(df['mask_file_names']):\n",
    "        source = os.path.join(disas_post_mask, df.iloc[idx]['mask_file_names'])\n",
    "        destination = os.path.join(disas_class_path, df.iloc[idx]['damage'])\n",
    "        if os.path.exists(destination):\n",
    "            pass\n",
    "        else:\n",
    "            print( \"Creating dir for \" , df.iloc[idx]['damage'])\n",
    "            os.mkdir(destination)\n",
    "        \n",
    "        if os.path.exists(source):\n",
    "            shutil.copy(source, destination)\n",
    "    print(\"Finshed moving the mask files to class folder for \", disaster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b30aa374-7899-4ef0-9918-b51afcad3cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask_file_names\n",
       "hurricane-michael_00000000_pre_disaster_dc6cb95a-4105-4cff-ae51-562fe8b949e5.png     1\n",
       "hurricane-michael_00000406_post_disaster_193ea56b-13a9-411b-bf88-b1fd467385e7.png    1\n",
       "hurricane-michael_00000406_post_disaster_92cd376a-8b4f-495b-997c-7da00e8da079.png    1\n",
       "hurricane-michael_00000406_post_disaster_cda45860-d351-449c-936f-d0cd97fd4bc6.png    1\n",
       "hurricane-michael_00000406_post_disaster_b773e571-00a3-4c89-9138-2d1c54bbf785.png    1\n",
       "                                                                                    ..\n",
       "hurricane-michael_00000214_post_disaster_b58e6d77-f171-435e-a91e-395b1e669647.png    1\n",
       "hurricane-michael_00000214_post_disaster_1b0540b4-8ed2-4ef3-b00d-b7707fb20944.png    1\n",
       "hurricane-michael_00000214_post_disaster_cae1b198-0727-47d8-b67b-b71e8b15c764.png    1\n",
       "hurricane-michael_00000214_post_disaster_46a9778c-3287-42a8-bfe2-16f5e126d378.png    1\n",
       "hurricane-michael_00000549_pre_disaster_e89ad57f-2670-495e-8fd4-30467a3fb371.png     1\n",
       "Name: count, Length: 45372, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hmc_train['dataset'].value_counts()\n",
    "df_hmc_train['is_post_image'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd8da950-0ce3-40d3-b4e1-c1d5bd7121c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "hold    14316\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hmc_hold['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8306f734-6950-4e7b-892d-c84522aebdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "test    11314\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hmc_test['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5477b0ff-da57-4130-98dc-d66e9c096548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/train/disaster/hurricane-michael/masks/post\n",
      "Destination root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/train/disaster/hurricane-michael/class/post\n",
      "Started moving the mask files to class folder for  hurricane-michael\n",
      "Finshed moving the mask files to class folder for  hurricane-michael\n"
     ]
    }
   ],
   "source": [
    "sort_masks_by_class(train_out_dir, df_hmc_train, 'hurricane-michael')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ebdce64c-7a10-475d-8aab-5521f13410fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/hold/disaster/hurricane-michael/masks/post\n",
      "Destination root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/hold/disaster/hurricane-michael/class/post\n",
      "Started moving the mask files to class folder for  hurricane-michael\n",
      "Creating dir for  no-damage\n",
      "Creating dir for  un-classified\n",
      "Creating dir for  major-damage\n",
      "Creating dir for  destroyed\n",
      "Creating dir for  minor-damage\n",
      "Finshed moving the mask files to class folder for  hurricane-michael\n"
     ]
    }
   ],
   "source": [
    "sort_masks_by_class(hold_out_dir, df_hmc_hold, 'hurricane-michael')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f1f16772-47af-401b-b108-591142ec72b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/test/disaster/hurricane-michael/masks/post\n",
      "Destination root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/test/disaster/hurricane-michael/class/post\n",
      "Started moving the mask files to class folder for  hurricane-michael\n",
      "Creating dir for  major-damage\n",
      "Creating dir for  no-damage\n",
      "Creating dir for  minor-damage\n",
      "Creating dir for  destroyed\n",
      "Creating dir for  un-classified\n",
      "Finshed moving the mask files to class folder for  hurricane-michael\n"
     ]
    }
   ],
   "source": [
    "sort_masks_by_class(test_out_dir, df_hmc_test, 'hurricane-michael')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "411706e3-bcc8-4936-beed-8cf4e941d752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/train/disaster/hurricane-michael/masks/post\n",
      "Destination root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/train/disaster/hurricane-michael/class/post\n",
      "Started moving the mask files to class folder for  hurricane-michael\n",
      "Creating dir for  pre\n",
      "Finshed moving the mask files to class folder for  hurricane-michael\n",
      "Source root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/hold/disaster/hurricane-michael/masks/post\n",
      "Destination root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/hold/disaster/hurricane-michael/class/post\n",
      "Started moving the mask files to class folder for  hurricane-michael\n",
      "Creating dir for  pre\n",
      "Finshed moving the mask files to class folder for  hurricane-michael\n",
      "Source root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/test/disaster/hurricane-michael/masks/post\n",
      "Destination root :  /Users/yaminigotimukul/DataScience/Berekley/Semesters/Spring_2024/repo/alivio/data/xview_building_damage/challenge/test/disaster/hurricane-michael/class/post\n",
      "Started moving the mask files to class folder for  hurricane-michael\n",
      "Creating dir for  pre\n",
      "Finshed moving the mask files to class folder for  hurricane-michael\n"
     ]
    }
   ],
   "source": [
    "sort_masks_by_class(train_out_dir, df_hmc_train, 'hurricane-michael', post=False)\n",
    "sort_masks_by_class(hold_out_dir, df_hmc_hold, 'hurricane-michael', post=False)\n",
    "sort_masks_by_class(test_out_dir, df_hmc_test, 'hurricane-michael', post=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
